{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4acc3ec4",
   "metadata": {
    "papermill": {
     "duration": 0.006727,
     "end_time": "2025-10-23T18:43:10.340570",
     "exception": false,
     "start_time": "2025-10-23T18:43:10.333843",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# üåê Spatio-Temporal GNN Phase 4 - Multi-Scenario Climate Prediction\n",
    "\n",
    "**Complete STGNN notebook with multi-scenario training across 18 CMIP6 institutions**\n",
    "\n",
    "---\n",
    "\n",
    "## üìã Features\n",
    "- ‚úÖ **Spatio-Temporal Graph Neural Network** (unified spatial-temporal modeling)\n",
    "- ‚úÖ **Temporal Graph Convolutions** for time-aware processing\n",
    "- ‚úÖ **Multi-Scenario Training** (historical + SSP126/245/370/585)\n",
    "- ‚úÖ **Stratified Splitting** (all scenarios in train/val/test)\n",
    "- ‚úÖ **8-neighbor Grid Connectivity**\n",
    "- ‚úÖ **Scenario Embeddings** as 8th input channel\n",
    "- ‚úÖ Same dataset as ConvLSTM (7 vars + scenario ‚Üí precipitation)\n",
    "- ‚úÖ Memory-efficient for 6GB GPU\n",
    "- ‚úÖ Multi-institution support\n",
    "\n",
    "## üéØ STGNN Architecture\n",
    "- **Spatial**: Graph Convolution (8-neighbor)\n",
    "- **Temporal**: Temporal Convolution (causal)\n",
    "- **Combined**: Spatio-Temporal Blocks\n",
    "- **Input**: 12 timesteps √ó 8 channels √ó 9√ó19 spatial\n",
    "- **Output**: 3 timesteps √ó 1 variable √ó 9√ó19 spatial\n",
    "\n",
    "## üî¨ Why STGNN?\n",
    "- **Unified Processing**: Spatial + temporal in one operation\n",
    "- **Better Efficiency**: No separate GRU overhead\n",
    "- **Parallel Processing**: Temporal convolutions are parallelizable\n",
    "- **Climate-Appropriate**: Captures spatio-temporal patterns naturally\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eeef592",
   "metadata": {
    "papermill": {
     "duration": 0.004865,
     "end_time": "2025-10-23T18:43:10.350913",
     "exception": false,
     "start_time": "2025-10-23T18:43:10.346048",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## üì¶ Section 1: Install & Import Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "437cf271",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-23T18:43:10.362534Z",
     "iopub.status.busy": "2025-10-23T18:43:10.362237Z",
     "iopub.status.idle": "2025-10-23T18:43:19.173633Z",
     "shell.execute_reply": "2025-10-23T18:43:19.172593Z"
    },
    "papermill": {
     "duration": 8.819154,
     "end_time": "2025-10-23T18:43:19.175280",
     "exception": false,
     "start_time": "2025-10-23T18:43:10.356126",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in links: https://data.pyg.org/whl/torch-2.6.0+cu124.html\r\n",
      "Collecting torch-geometric\r\n",
      "  Downloading torch_geometric-2.7.0-py3-none-any.whl.metadata (63 kB)\r\n",
      "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m63.7/63.7 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hCollecting torch-scatter\r\n",
      "  Downloading https://data.pyg.org/whl/torch-2.6.0%2Bcu124/torch_scatter-2.1.2%2Bpt26cu124-cp311-cp311-linux_x86_64.whl (10.8 MB)\r\n",
      "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m10.8/10.8 MB\u001b[0m \u001b[31m79.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hCollecting torch-sparse\r\n",
      "  Downloading https://data.pyg.org/whl/torch-2.6.0%2Bcu124/torch_sparse-0.6.18%2Bpt26cu124-cp311-cp311-linux_x86_64.whl (5.0 MB)\r\n",
      "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m5.0/5.0 MB\u001b[0m \u001b[31m83.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hCollecting torch-cluster\r\n",
      "  Downloading https://data.pyg.org/whl/torch-2.6.0%2Bcu124/torch_cluster-1.6.3%2Bpt26cu124-cp311-cp311-linux_x86_64.whl (3.4 MB)\r\n",
      "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m34.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hCollecting torch-spline-conv\r\n",
      "  Downloading https://data.pyg.org/whl/torch-2.6.0%2Bcu124/torch_spline_conv-1.2.2%2Bpt26cu124-cp311-cp311-linux_x86_64.whl (1.0 MB)\r\n",
      "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m15.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hRequirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (3.12.15)\r\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (2025.9.0)\r\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (3.1.6)\r\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (1.26.4)\r\n",
      "Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (7.1.0)\r\n",
      "Requirement already satisfied: pyparsing in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (3.0.9)\r\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (2.32.5)\r\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (4.67.1)\r\n",
      "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (3.5.0)\r\n",
      "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from torch-sparse) (1.15.3)\r\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (2.6.1)\r\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (1.4.0)\r\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (25.3.0)\r\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (1.7.0)\r\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (6.6.4)\r\n",
      "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (0.3.2)\r\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (1.20.1)\r\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch-geometric) (3.0.2)\r\n",
      "Requirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy->torch-geometric) (1.3.8)\r\n",
      "Requirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy->torch-geometric) (1.2.4)\r\n",
      "Requirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy->torch-geometric) (0.1.1)\r\n",
      "Requirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy->torch-geometric) (2025.2.0)\r\n",
      "Requirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy->torch-geometric) (2022.2.0)\r\n",
      "Requirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy->torch-geometric) (2.4.1)\r\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->torch-geometric) (3.4.3)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->torch-geometric) (3.10)\r\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->torch-geometric) (2.5.0)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->torch-geometric) (2025.8.3)\r\n",
      "Requirement already satisfied: typing-extensions>=4.2 in /usr/local/lib/python3.11/dist-packages (from aiosignal>=1.4.0->aiohttp->torch-geometric) (4.15.0)\r\n",
      "Requirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->torch-geometric) (2024.2.0)\r\n",
      "Requirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->torch-geometric) (2022.2.0)\r\n",
      "Requirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy->torch-geometric) (1.4.0)\r\n",
      "Requirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy->torch-geometric) (2024.2.0)\r\n",
      "Requirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy->torch-geometric) (2024.2.0)\r\n",
      "Downloading torch_geometric-2.7.0-py3-none-any.whl (1.3 MB)\r\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m22.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hInstalling collected packages: torch-spline-conv, torch-scatter, torch-sparse, torch-geometric, torch-cluster\r\n",
      "Successfully installed torch-cluster-1.6.3+pt26cu124 torch-geometric-2.7.0 torch-scatter-2.1.2+pt26cu124 torch-sparse-0.6.18+pt26cu124 torch-spline-conv-1.2.2+pt26cu124\r\n"
     ]
    }
   ],
   "source": [
    "!pip install torch-geometric torch-scatter torch-sparse torch-cluster torch-spline-conv \\\n",
    "  -f https://data.pyg.org/whl/torch-2.6.0+cu124.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ebf8806c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-23T18:43:19.191861Z",
     "iopub.status.busy": "2025-10-23T18:43:19.190922Z",
     "iopub.status.idle": "2025-10-23T18:43:37.831229Z",
     "shell.execute_reply": "2025-10-23T18:43:37.830005Z"
    },
    "papermill": {
     "duration": 18.649985,
     "end_time": "2025-10-23T18:43:37.832701",
     "exception": false,
     "start_time": "2025-10-23T18:43:19.182716",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ PyTorch Geometric is working!\n"
     ]
    }
   ],
   "source": [
    "from torch_geometric.nn import GCNConv\n",
    "print(\"‚úÖ PyTorch Geometric is working!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6cadfe52",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-23T18:43:37.848510Z",
     "iopub.status.busy": "2025-10-23T18:43:37.847941Z",
     "iopub.status.idle": "2025-10-23T18:43:39.880709Z",
     "shell.execute_reply": "2025-10-23T18:43:39.879697Z"
    },
    "papermill": {
     "duration": 2.042425,
     "end_time": "2025-10-23T18:43:39.882329",
     "exception": false,
     "start_time": "2025-10-23T18:43:37.839904",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Using device: cpu\n",
      "‚ö†Ô∏è WARNING: No GPU detected - training will be VERY slow!\n",
      "‚úÖ All imports successful!\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# üì¶ SECTION 1: INSTALL & IMPORT DEPENDENCIES\n",
    "# ============================================================================\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import glob\n",
    "import time\n",
    "import json\n",
    "import warnings\n",
    "import gc\n",
    "from datetime import datetime\n",
    "from typing import Dict, List, Tuple, Optional\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "from scipy.ndimage import zoom\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch_geometric.nn import GCNConv\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# GPU setup\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"üöÄ Using device: {device}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"üìä GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"üíæ GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è WARNING: No GPU detected - training will be VERY slow!\")\n",
    "\n",
    "sys.stdout.flush()\n",
    "print(\"‚úÖ All imports successful!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75e9debc",
   "metadata": {
    "papermill": {
     "duration": 0.007082,
     "end_time": "2025-10-23T18:43:39.896618",
     "exception": false,
     "start_time": "2025-10-23T18:43:39.889536",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## ‚öôÔ∏è Section 2: Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4e96b563",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-23T18:43:39.912936Z",
     "iopub.status.busy": "2025-10-23T18:43:39.912090Z",
     "iopub.status.idle": "2025-10-23T18:43:39.956353Z",
     "shell.execute_reply": "2025-10-23T18:43:39.955043Z"
    },
    "papermill": {
     "duration": 0.054218,
     "end_time": "2025-10-23T18:43:39.957849",
     "exception": false,
     "start_time": "2025-10-23T18:43:39.903631",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "‚öôÔ∏è STGNN PHASE 4 - CONFIGURATION\n",
      "================================================================================\n",
      "\n",
      "üìÅ Creating output directories...\n",
      "‚úÖ Base: /kaggle/working/stgnn_phase4_multiscenario_results\n",
      "   ‚îú‚îÄ checkpoints/\n",
      "   ‚îú‚îÄ logs/\n",
      "   ‚îú‚îÄ plots/\n",
      "   ‚îú‚îÄ metrics/\n",
      "   ‚îî‚îÄ All directories created!\n",
      "\n",
      "‚úÖ Write permissions verified!\n",
      "\n",
      "================================================================================\n",
      "üìä CONFIGURATION SUMMARY\n",
      "================================================================================\n",
      "\n",
      "üìÇ PATHS:\n",
      "   Input:  /kaggle/input/climate-dataset/Datasets/inputs/input4mips\n",
      "   Output: /kaggle/input/climate-dataset/Datasets/outputs/CMIP6\n",
      "   Results: /kaggle/working/stgnn_phase4_multiscenario_results\n",
      "\n",
      "üó∫Ô∏è  SPATIAL & TRAINING:\n",
      "   Grid size:  9√ó19\n",
      "   Batch size: 1\n",
      "   Epochs:     2\n",
      "   Smoke test: True\n",
      "\n",
      "üè¢ DATA:\n",
      "   Institutions: 18\n",
      "   Scenarios:    ['historical', 'ssp126', 'ssp245', 'ssp370', 'ssp585']\n",
      "   Input vars:   7\n",
      "   Input channels: 8 (7 vars + scenario)\n",
      "   Output var:   pr\n",
      "\n",
      "üèóÔ∏è  MODEL (STGNN):\n",
      "   Hidden dim:    64\n",
      "   STGNN blocks:  3\n",
      "   Temporal kernel: 3\n",
      "   Spatial kernel:  2\n",
      "   Dropout:       0.2\n",
      "\n",
      "================================================================================\n",
      "üîç VERIFYING INPUT PATHS\n",
      "================================================================================\n",
      "‚úÖ Input data path exists\n",
      "‚úÖ Output data path exists\n",
      "\n",
      "================================================================================\n",
      "‚úÖ CONFIGURATION COMPLETE!\n",
      "================================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# ‚öôÔ∏è SECTION 2: CONFIGURATION - FIXED FOR KAGGLE\n",
    "# =============================================================================\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"‚öôÔ∏è STGNN PHASE 4 - CONFIGURATION\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# =============================================================================\n",
    "# üìÇ PATHS\n",
    "# =============================================================================\n",
    "INPUT_DATA_DIR = \"/kaggle/input/climate-dataset/Datasets/inputs/input4mips\"\n",
    "OUTPUT_DATA_DIR = \"/kaggle/input/climate-dataset/Datasets/outputs/CMIP6\"\n",
    "\n",
    "# ‚úÖ Use absolute path in /kaggle/working/\n",
    "OUTPUT_DIR = \"/kaggle/working/stgnn_phase4_multiscenario_results\"\n",
    "\n",
    "# =============================================================================\n",
    "# üìÅ CREATE OUTPUT DIRECTORIES\n",
    "# =============================================================================\n",
    "print(\"\\nüìÅ Creating output directories...\")\n",
    "\n",
    "# Define all subdirectories\n",
    "SUBDIRS = [\"checkpoints\", \"logs\", \"plots\", \"metrics\"]\n",
    "\n",
    "# Create base directory\n",
    "Path(OUTPUT_DIR).mkdir(parents=True, exist_ok=True)\n",
    "print(f\"‚úÖ Base: {OUTPUT_DIR}\")\n",
    "\n",
    "# Create subdirectories\n",
    "for subdir in SUBDIRS:\n",
    "    subdir_path = Path(OUTPUT_DIR) / subdir\n",
    "    subdir_path.mkdir(parents=True, exist_ok=True)\n",
    "    print(f\"   ‚îú‚îÄ {subdir}/\")\n",
    "\n",
    "print(\"   ‚îî‚îÄ All directories created!\")\n",
    "\n",
    "# Verify write permissions\n",
    "test_file = Path(OUTPUT_DIR) / \".write_test\"\n",
    "try:\n",
    "    test_file.write_text(\"test\")\n",
    "    test_file.unlink()\n",
    "    print(\"\\n‚úÖ Write permissions verified!\")\n",
    "except Exception as e:\n",
    "    print(f\"\\n‚ùå ERROR: Cannot write to output directory!\")\n",
    "    print(f\"   Error: {e}\")\n",
    "    raise RuntimeError(f\"Output directory not writable: {OUTPUT_DIR}\")\n",
    "\n",
    "# =============================================================================\n",
    "# ‚öôÔ∏è TRAINING SETTINGS\n",
    "# =============================================================================\n",
    "SMOKE_TEST = True              # True = 2 institutions, 2 epochs for testing\n",
    "RESUME_IF_MODEL_EXISTS = True  # Skip already trained models\n",
    "SKIP_TRAINING = False          # Set True to only load results\n",
    "\n",
    "# =============================================================================\n",
    "# üß† MEMORY SETTINGS\n",
    "# =============================================================================\n",
    "TARGET_SPATIAL_H = 9\n",
    "TARGET_SPATIAL_W = 19\n",
    "BATCH_SIZE = 1\n",
    "EPOCHS = 2 if SMOKE_TEST else 15\n",
    "\n",
    "# =============================================================================\n",
    "# üèóÔ∏è MODEL SETTINGS (STGNN)\n",
    "# =============================================================================\n",
    "HIDDEN_DIM = 64\n",
    "STGNN_BLOCKS = 3              # Number of spatio-temporal blocks\n",
    "TEMPORAL_KERNEL_SIZE = 3       # Temporal convolution kernel\n",
    "SPATIAL_KERNEL_SIZE = 2        # Graph conv layers per block\n",
    "DROPOUT = 0.2\n",
    "\n",
    "# =============================================================================\n",
    "# üè¢ INSTITUTIONS & SCENARIOS\n",
    "# =============================================================================\n",
    "# All 18 institutions\n",
    "ALL_INSTITUTIONS = [\n",
    "    \"AWI-CM-1-1-MR\", \"BCC-CSM2-MR\", \"CAS-ESM2-0\", \"CESM2\",\n",
    "    \"CESM2-WACCM\", \"CMCC-CM2-SR5\", \"CMCC-ESM2\", \"CNRM-CM6-1-HR\",\n",
    "    \"EC-Earth3\", \"EC-Earth3-Veg\", \"EC-Earth3-Veg-LR\", \"FGOALS-f3-L\",\n",
    "    \"GFDL-ESM4\", \"INM-CM4-8\", \"INM-CM5-0\", \"MPI-ESM1-2-HR\",\n",
    "    \"MRI-ESM2-0\", \"TaiESM1\"\n",
    "]\n",
    "\n",
    "# Scenarios with numeric IDs\n",
    "SCENARIOS = {\n",
    "    'historical': 0,\n",
    "    'ssp126': 1,\n",
    "    'ssp245': 2,\n",
    "    'ssp370': 3,\n",
    "    'ssp585': 4\n",
    "}\n",
    "\n",
    "# =============================================================================\n",
    "# üìä DATA VARIABLES\n",
    "# =============================================================================\n",
    "# Input variables (7 climate forcing variables)\n",
    "INPUT_VARIABLES = [\n",
    "    'BC_anthro_fires', 'BC_no_fires',\n",
    "    'CH4_anthro_fires', 'CH4_no_fires',\n",
    "    'CO2_sum',\n",
    "    'SO2_anthro_fires', 'SO2_no_fires'\n",
    "]\n",
    "\n",
    "# Total input channels: 7 variables + 1 scenario channel = 8\n",
    "NUM_INPUT_CHANNELS = 8\n",
    "OUTPUT_VARIABLE = 'pr'\n",
    "\n",
    "# =============================================================================\n",
    "# ‚úÖ CONFIGURATION SUMMARY\n",
    "# =============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üìä CONFIGURATION SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"\\nüìÇ PATHS:\")\n",
    "print(f\"   Input:  {INPUT_DATA_DIR}\")\n",
    "print(f\"   Output: {OUTPUT_DATA_DIR}\")\n",
    "print(f\"   Results: {OUTPUT_DIR}\")\n",
    "\n",
    "print(f\"\\nüó∫Ô∏è  SPATIAL & TRAINING:\")\n",
    "print(f\"   Grid size:  {TARGET_SPATIAL_H}√ó{TARGET_SPATIAL_W}\")\n",
    "print(f\"   Batch size: {BATCH_SIZE}\")\n",
    "print(f\"   Epochs:     {EPOCHS}\")\n",
    "print(f\"   Smoke test: {SMOKE_TEST}\")\n",
    "\n",
    "print(f\"\\nüè¢ DATA:\")\n",
    "print(f\"   Institutions: {len(ALL_INSTITUTIONS)}\")\n",
    "print(f\"   Scenarios:    {list(SCENARIOS.keys())}\")\n",
    "print(f\"   Input vars:   {len(INPUT_VARIABLES)}\")\n",
    "print(f\"   Input channels: {NUM_INPUT_CHANNELS} (7 vars + scenario)\")\n",
    "print(f\"   Output var:   {OUTPUT_VARIABLE}\")\n",
    "\n",
    "print(f\"\\nüèóÔ∏è  MODEL (STGNN):\")\n",
    "print(f\"   Hidden dim:    {HIDDEN_DIM}\")\n",
    "print(f\"   STGNN blocks:  {STGNN_BLOCKS}\")\n",
    "print(f\"   Temporal kernel: {TEMPORAL_KERNEL_SIZE}\")\n",
    "print(f\"   Spatial kernel:  {SPATIAL_KERNEL_SIZE}\")\n",
    "print(f\"   Dropout:       {DROPOUT}\")\n",
    "\n",
    "# =============================================================================\n",
    "# üîç VERIFY INPUT PATHS\n",
    "# =============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üîç VERIFYING INPUT PATHS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "if os.path.exists(INPUT_DATA_DIR):\n",
    "    print(f\"‚úÖ Input data path exists\")\n",
    "else:\n",
    "    print(f\"‚ùå ERROR: Input path not found!\")\n",
    "    print(f\"   Path: {INPUT_DATA_DIR}\")\n",
    "    print(f\"   ‚ö†Ô∏è  Make sure 'climate-dataset' is attached!\")\n",
    "\n",
    "if os.path.exists(OUTPUT_DATA_DIR):\n",
    "    print(f\"‚úÖ Output data path exists\")\n",
    "else:\n",
    "    print(f\"‚ùå ERROR: Output path not found!\")\n",
    "    print(f\"   Path: {OUTPUT_DATA_DIR}\")\n",
    "    print(f\"   ‚ö†Ô∏è  Make sure 'climate-dataset' is attached!\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"‚úÖ CONFIGURATION COMPLETE!\")\n",
    "print(\"=\"*80 + \"\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "703fb04e",
   "metadata": {
    "papermill": {
     "duration": 0.006805,
     "end_time": "2025-10-23T18:43:39.971947",
     "exception": false,
     "start_time": "2025-10-23T18:43:39.965142",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## üéØ **Key Changes:**\n",
    "\n",
    "1. ‚úÖ **Absolute path**: `/kaggle/working/stgnn_phase4_multiscenario_results`\n",
    "2. ‚úÖ **All subdirectories created**: checkpoints, logs, plots, metrics\n",
    "3. ‚úÖ **Write permission verification**\n",
    "4. ‚úÖ **Better organized sections**\n",
    "5. ‚úÖ **Comprehensive summary output**\n",
    "6. ‚úÖ **Path verification**\n",
    "7. ‚úÖ **Professional formatting with emojis**\n",
    "\n",
    "---\n",
    "\n",
    "## üìÅ **Directory Structure Created:**\n",
    "```\n",
    "/kaggle/working/stgnn_phase4_multiscenario_results/\n",
    "‚îú‚îÄ‚îÄ checkpoints/    ‚Üê Model .pt files\n",
    "‚îú‚îÄ‚îÄ logs/          ‚Üê JSON training logs\n",
    "‚îú‚îÄ‚îÄ plots/         ‚Üê Visualization PNGs\n",
    "‚îî‚îÄ‚îÄ metrics/       ‚Üê CSV epoch metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2550b61",
   "metadata": {
    "papermill": {
     "duration": 0.006623,
     "end_time": "2025-10-23T18:43:39.985469",
     "exception": false,
     "start_time": "2025-10-23T18:43:39.978846",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## üó∫Ô∏è Section 3: Graph Construction Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a760965f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-23T18:43:40.000630Z",
     "iopub.status.busy": "2025-10-23T18:43:40.000285Z",
     "iopub.status.idle": "2025-10-23T18:43:40.012941Z",
     "shell.execute_reply": "2025-10-23T18:43:40.011732Z"
    },
    "papermill": {
     "duration": 0.022371,
     "end_time": "2025-10-23T18:43:40.014641",
     "exception": false,
     "start_time": "2025-10-23T18:43:39.992270",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Graph utilities loaded\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# üó∫Ô∏è SECTION 3: GRAPH CONSTRUCTION\n",
    "# ============================================================================\n",
    "\n",
    "def build_grid_graph_8neighbor(height: int, width: int) -> torch.Tensor:\n",
    "    \"\"\"Build 8-neighbor connectivity graph for grid.\"\"\"\n",
    "    edges = []\n",
    "    \n",
    "    def pos_to_idx(i, j):\n",
    "        return i * width + j\n",
    "    \n",
    "    # 8 directions: N, S, E, W, NE, NW, SE, SW\n",
    "    directions = [\n",
    "        (-1, 0), (1, 0), (0, -1), (0, 1),\n",
    "        (-1, -1), (-1, 1), (1, -1), (1, 1)\n",
    "    ]\n",
    "    \n",
    "    for i in range(height):\n",
    "        for j in range(width):\n",
    "            src_idx = pos_to_idx(i, j)\n",
    "            for di, dj in directions:\n",
    "                ni, nj = i + di, j + dj\n",
    "                if 0 <= ni < height and 0 <= nj < width:\n",
    "                    dst_idx = pos_to_idx(ni, nj)\n",
    "                    edges.append([src_idx, dst_idx])\n",
    "    \n",
    "    edge_index = torch.tensor(edges, dtype=torch.long).t().contiguous()\n",
    "    print(f\"üìä Graph: {height}√ó{width} = {height*width} nodes, {edge_index.shape[1]} edges\")\n",
    "    print(f\"   Avg degree: {edge_index.shape[1] / (height*width):.2f}\")\n",
    "    return edge_index\n",
    "\n",
    "\n",
    "def add_self_loops(edge_index: torch.Tensor, num_nodes: int) -> torch.Tensor:\n",
    "    \"\"\"Add self-loops to graph.\"\"\"\n",
    "    self_loops = torch.stack([torch.arange(num_nodes), torch.arange(num_nodes)], dim=0)\n",
    "    return torch.cat([edge_index, self_loops], dim=1)\n",
    "\n",
    "\n",
    "def get_positional_encoding(height: int, width: int, embed_dim: int) -> torch.Tensor:\n",
    "    \"\"\"Generate 2D positional encoding.\"\"\"\n",
    "    y_coords = torch.linspace(0, 1, height)\n",
    "    x_coords = torch.linspace(0, 1, width)\n",
    "    yy, xx = torch.meshgrid(y_coords, x_coords, indexing='ij')\n",
    "    coords = torch.stack([yy.flatten(), xx.flatten()], dim=1)\n",
    "    \n",
    "    pos_encoding = torch.zeros(height * width, embed_dim)\n",
    "    for i in range(embed_dim // 2):\n",
    "        freq = 1.0 / (10000 ** (2 * i / embed_dim))\n",
    "        pos_encoding[:, 2*i] = torch.sin(coords[:, 0] * freq)\n",
    "        pos_encoding[:, 2*i + 1] = torch.cos(coords[:, 1] * freq)\n",
    "    \n",
    "    return pos_encoding\n",
    "\n",
    "\n",
    "print(\"‚úÖ Graph utilities loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "944ccd8e",
   "metadata": {
    "papermill": {
     "duration": 0.007256,
     "end_time": "2025-10-23T18:43:40.029248",
     "exception": false,
     "start_time": "2025-10-23T18:43:40.021992",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## üìä Section 4: Multi-Scenario Data Loader\n",
    "\n",
    "*Same as previous - unchanged*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5558e29e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-23T18:43:40.046514Z",
     "iopub.status.busy": "2025-10-23T18:43:40.046128Z",
     "iopub.status.idle": "2025-10-23T18:43:40.086337Z",
     "shell.execute_reply": "2025-10-23T18:43:40.085262Z"
    },
    "papermill": {
     "duration": 0.05027,
     "end_time": "2025-10-23T18:43:40.087666",
     "exception": false,
     "start_time": "2025-10-23T18:43:40.037396",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Multi-scenario data loader defined\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# üìä SECTION 4: MULTI-SCENARIO DATA LOADER (SAME AS BEFORE)\n",
    "# ============================================================================\n",
    "\n",
    "class ClimateDatasetWithScenario(Dataset):\n",
    "    \"\"\"PyTorch dataset with scenario information.\"\"\"\n",
    "    \n",
    "    def __init__(self, X: np.ndarray, y: np.ndarray, scenarios: np.ndarray):\n",
    "        self.X = torch.from_numpy(X.astype(np.float32))\n",
    "        self.y = torch.from_numpy(y.astype(np.float32))\n",
    "        self.scenarios = torch.from_numpy(scenarios.astype(np.float32))\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.X.shape[0]\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx], self.scenarios[idx]\n",
    "\n",
    "\n",
    "class MultiScenarioDataLoader:\n",
    "    \"\"\"Multi-Scenario Data Loader for STGNN.\"\"\"\n",
    "    \n",
    "    def __init__(self, target_h: int = 9, target_w: int = 19):\n",
    "        self.input_base = INPUT_DATA_DIR\n",
    "        self.output_base = OUTPUT_DATA_DIR\n",
    "        self.target_h = target_h\n",
    "        self.target_w = target_w\n",
    "        self.scalers = {}\n",
    "        self.scenarios = SCENARIOS\n",
    "        self.input_variables = INPUT_VARIABLES\n",
    "        \n",
    "        self.variable_dirs = {\n",
    "            'BC_anthro_fires': 'BC_sum', 'BC_no_fires': 'BC_sum',\n",
    "            'CH4_anthro_fires': 'CH4_sum', 'CH4_no_fires': 'CH4_sum',\n",
    "            'CO2_sum': 'CO2_sum',\n",
    "            'SO2_anthro_fires': 'SO2_sum', 'SO2_no_fires': 'SO2_sum'\n",
    "        }\n",
    "        self.output_variable = OUTPUT_VARIABLE\n",
    "        print(f\"üìä DataLoader initialized: {target_h}√ó{target_w}\")\n",
    "    \n",
    "    def _matches_variable_pattern(self, filename: str, variable: str) -> bool:\n",
    "        \"\"\"Pattern matching for filenames.\"\"\"\n",
    "        f = filename.lower()\n",
    "        patterns = {\n",
    "            'BC_anthro_fires': (['bc', 'anthro'], ['fire']),\n",
    "            'BC_no_fires': (['bc', 'no'], ['fire']),\n",
    "            'CH4_anthro_fires': (['ch4', 'anthro'], ['fire']),\n",
    "            'CH4_no_fires': (['ch4', 'no'], ['fire']),\n",
    "            'CO2_sum': (['co2'], []),\n",
    "            'SO2_anthro_fires': (['so2', 'anthro'], ['fire']),\n",
    "            'SO2_no_fires': (['so2', 'no'], ['fire'])\n",
    "        }\n",
    "        \n",
    "        if variable in patterns:\n",
    "            req, opt = patterns[variable]\n",
    "            has_req = all(p in f for p in req)\n",
    "            if 'no' in req:\n",
    "                has_req = has_req and 'anthro' not in f\n",
    "            if opt:\n",
    "                return has_req and any(p in f for p in opt)\n",
    "            return has_req\n",
    "        return False\n",
    "    \n",
    "    def _is_file_readable(self, file_path: str) -> bool:\n",
    "        \"\"\"Check if file can be opened.\"\"\"\n",
    "        try:\n",
    "            if not os.path.exists(file_path) or os.path.getsize(file_path) < 1000:\n",
    "                return False\n",
    "            with xr.open_dataset(file_path, decode_times=False) as ds:\n",
    "                _ = list(ds.data_vars.keys())\n",
    "            return True\n",
    "        except:\n",
    "            return False\n",
    "    \n",
    "    def discover_files_multi_scenario(self, institution: str):\n",
    "        \"\"\"Discover files for all scenarios.\"\"\"\n",
    "        print(f\"üîç Discovering files for {institution}...\")\n",
    "        all_scenario_files = {}\n",
    "        \n",
    "        for scenario in self.scenarios.keys():\n",
    "            print(f\"   üìÅ {scenario}\")\n",
    "            results_inputs = {}\n",
    "            results_outputs = {}\n",
    "            \n",
    "            # Input files\n",
    "            for var in self.input_variables:\n",
    "                folder = self.variable_dirs.get(var, var)\n",
    "                path = os.path.join(self.input_base, scenario, folder)\n",
    "                \n",
    "                if not os.path.exists(path):\n",
    "                    continue\n",
    "                \n",
    "                found = []\n",
    "                for root, dirs, files in os.walk(path):\n",
    "                    for fname in files:\n",
    "                        if fname.endswith('.nc') and self._matches_variable_pattern(fname, var):\n",
    "                            fpath = os.path.join(root, fname)\n",
    "                            if self._is_file_readable(fpath):\n",
    "                                found.append(fpath)\n",
    "                \n",
    "                if found:\n",
    "                    results_inputs[var] = found\n",
    "                    print(f\"      ‚úì {var}: {len(found)} files\")\n",
    "            \n",
    "            # Output files\n",
    "            out_path = os.path.join(self.output_base, institution, scenario, self.output_variable)\n",
    "            if os.path.exists(out_path):\n",
    "                out_files = []\n",
    "                for root, dirs, files in os.walk(out_path):\n",
    "                    for fname in files:\n",
    "                        if fname.endswith('.nc'):\n",
    "                            fpath = os.path.join(root, fname)\n",
    "                            if self._is_file_readable(fpath):\n",
    "                                out_files.append(fpath)\n",
    "                \n",
    "                if out_files:\n",
    "                    results_outputs[self.output_variable] = out_files\n",
    "                    print(f\"      ‚úì {self.output_variable}: {len(out_files)} files\")\n",
    "            \n",
    "            all_scenario_files[scenario] = {\"inputs\": results_inputs, \"outputs\": results_outputs}\n",
    "        \n",
    "        return all_scenario_files\n",
    "    \n",
    "    def downsample_spatial(self, arr, target_h, target_w):\n",
    "        \"\"\"Downsample using scipy.ndimage.zoom.\"\"\"\n",
    "        if arr.shape[1] == target_h and arr.shape[2] == target_w:\n",
    "            return arr\n",
    "        T, H, W = arr.shape\n",
    "        downsampled = zoom(arr, [1.0, target_h/H, target_w/W], order=1, mode='nearest')\n",
    "        return downsampled[:, :target_h, :target_w]\n",
    "    \n",
    "    def _load_netcdf_list(self, paths, var_hint=None):\n",
    "        \"\"\"Load and concatenate NetCDF files.\"\"\"\n",
    "        arrays = []\n",
    "        for p in sorted(paths):\n",
    "            try:\n",
    "                ds = xr.open_dataset(p, decode_times=False)\n",
    "                dvars = list(ds.data_vars.keys())\n",
    "                if not dvars:\n",
    "                    ds.close()\n",
    "                    continue\n",
    "                var = var_hint if (var_hint and var_hint in dvars) else dvars[0]\n",
    "                arr = ds[var].values\n",
    "                ds.close()\n",
    "                if arr.ndim == 2:\n",
    "                    arr = np.expand_dims(arr, 0)\n",
    "                elif arr.ndim > 3:\n",
    "                    arr = arr.reshape(-1, arr.shape[-2], arr.shape[-1])\n",
    "                arrays.append(np.nan_to_num(arr, 0.0, 0.0, 0.0))\n",
    "            except Exception:\n",
    "                continue\n",
    "        \n",
    "        if not arrays:\n",
    "            return np.zeros((0, 0, 0), dtype=float)\n",
    "        \n",
    "        try:\n",
    "            return np.concatenate(arrays, axis=0)\n",
    "        except:\n",
    "            mh = max(a.shape[1] for a in arrays)\n",
    "            mw = max(a.shape[2] for a in arrays)\n",
    "            padded = [np.pad(a, ((0,0), (0,mh-a.shape[1]), (0,mw-a.shape[2])), 'edge') for a in arrays]\n",
    "            return np.concatenate(padded, axis=0)\n",
    "    \n",
    "    def align_temporal_dimensions(self, var_data):\n",
    "        \"\"\"Align temporal dimensions.\"\"\"\n",
    "        if not var_data:\n",
    "            return var_data\n",
    "        times = {v: a.shape[0] for v, a in var_data.items()}\n",
    "        mt = min(times.values())\n",
    "        if mt != max(times.values()):\n",
    "            print(f\"      ‚ö†Ô∏è Aligning to {mt} timesteps\")\n",
    "            return {v: a[:mt] for v, a in var_data.items()}\n",
    "        return var_data\n",
    "    \n",
    "    def load_all_scenarios(self, all_files):\n",
    "        \"\"\"Load all scenario data.\"\"\"\n",
    "        print(\"üìä Loading scenarios...\")\n",
    "        all_data = {}\n",
    "        \n",
    "        for scenario, files in all_files.items():\n",
    "            print(f\"   üìÅ {scenario}\")\n",
    "            var_data = {}\n",
    "            \n",
    "            for var, paths in files.get(\"inputs\", {}).items():\n",
    "                if paths:\n",
    "                    arr = self._load_netcdf_list(paths)\n",
    "                    if arr.size > 0:\n",
    "                        var_data[var] = arr\n",
    "                        print(f\"      ‚úì {var}: {arr.shape}\")\n",
    "            \n",
    "            for var, paths in files.get(\"outputs\", {}).items():\n",
    "                if paths:\n",
    "                    arr = self._load_netcdf_list(paths)\n",
    "                    if arr.size > 0:\n",
    "                        var_data[var] = arr\n",
    "                        print(f\"      ‚úì {var}: {arr.shape}\")\n",
    "            \n",
    "            if var_data:\n",
    "                var_data = self.align_temporal_dimensions(var_data)\n",
    "                print(f\"      üîΩ Downsampling...\")\n",
    "                down_data = {}\n",
    "                for v, a in var_data.items():\n",
    "                    d = self.downsample_spatial(a, self.target_h, self.target_w)\n",
    "                    down_data[v] = d\n",
    "                    print(f\"         {v}: {a.shape} ‚Üí {d.shape}\")\n",
    "                all_data[scenario] = down_data\n",
    "        \n",
    "        print(f\"   ‚úÖ Loaded {len(all_data)} scenarios\")\n",
    "        return all_data\n",
    "    \n",
    "    def normalize_data(self, arr, var_name, fit=True):\n",
    "        \"\"\"Normalize data.\"\"\"\n",
    "        arr = np.nan_to_num(arr, 0.0, 0.0, 0.0)\n",
    "        flat = arr.reshape(-1, 1)\n",
    "        \n",
    "        if fit or var_name not in self.scalers:\n",
    "            scaler = MinMaxScaler()\n",
    "            try:\n",
    "                scaler.fit(flat)\n",
    "            except:\n",
    "                scaler.min_, scaler.scale_ = np.min(flat), 1.0\n",
    "            self.scalers[var_name] = scaler\n",
    "        else:\n",
    "            scaler = self.scalers[var_name]\n",
    "        \n",
    "        return np.nan_to_num(scaler.transform(flat).reshape(arr.shape), 0.0, 0.0, 0.0)\n",
    "    \n",
    "    def create_multi_scenario_sequences(self, all_data, seq_in=12, seq_out=3, stride=1, \n",
    "                                       train_ratio=0.7, val_ratio=0.15, batch_size=1):\n",
    "        \"\"\"Create sequences with stratified splitting.\"\"\"\n",
    "        print(\"üîÑ Creating sequences...\")\n",
    "        X_seqs, Y_seqs, sc_ids = [], [], []\n",
    "        \n",
    "        for scenario, var_data in all_data.items():\n",
    "            if not var_data:\n",
    "                continue\n",
    "            \n",
    "            sc_id = self.scenarios[scenario]\n",
    "            print(f\"   üì¶ {scenario} (id={sc_id})\")\n",
    "            \n",
    "            missing = [v for v in self.input_variables if v not in var_data]\n",
    "            if missing or self.output_variable not in var_data:\n",
    "                print(f\"      ‚ö†Ô∏è Skipping - missing data\")\n",
    "                continue\n",
    "            \n",
    "            # Normalize\n",
    "            norm = {v: self.normalize_data(var_data[v], v, True) for v in self.input_variables if v in var_data}\n",
    "            norm[self.output_variable] = self.normalize_data(var_data[self.output_variable], \n",
    "                                                             self.output_variable, True)\n",
    "            \n",
    "            # Stack\n",
    "            X_sc = np.stack([norm[v] for v in self.input_variables if v in norm], axis=1)\n",
    "            Y_sc = norm[self.output_variable]\n",
    "            \n",
    "            T = X_sc.shape[0]\n",
    "            n_samp = T - seq_in - seq_out + 1\n",
    "            \n",
    "            if n_samp <= 0:\n",
    "                print(f\"      ‚ö†Ô∏è Not enough timesteps\")\n",
    "                continue\n",
    "            \n",
    "            for start in range(0, n_samp, stride):\n",
    "                X_seqs.append(X_sc[start:start+seq_in])\n",
    "                Y_seqs.append(Y_sc[start+seq_in:start+seq_in+seq_out])\n",
    "                sc_ids.append(sc_id)\n",
    "            \n",
    "            print(f\"      ‚úì {n_samp} sequences\")\n",
    "        \n",
    "        if not X_seqs:\n",
    "            raise RuntimeError(\"‚ùå No sequences!\")\n",
    "        \n",
    "        X_all = np.stack(X_seqs, 0)\n",
    "        Y_all = np.stack(Y_seqs, 0)\n",
    "        sc_all = np.array(sc_ids)\n",
    "        \n",
    "        # Add scenario channel\n",
    "        N, T, C, H, W = X_all.shape\n",
    "        sc_ch = np.repeat(sc_all[:, None, None, None, None], T*H*W, 1).reshape(N, T, 1, H, W)\n",
    "        X_all = np.concatenate([X_all, sc_ch], 2)\n",
    "        Y_all = np.expand_dims(Y_all, 2)\n",
    "        \n",
    "        print(f\"   üß© Total: X={X_all.shape}, Y={Y_all.shape}\")\n",
    "        \n",
    "        # Stratified split\n",
    "        print(\"   üìä Stratified split...\")\n",
    "        X_tr, X_tmp, y_tr, y_tmp, sc_tr, sc_tmp = train_test_split(\n",
    "            X_all, Y_all, sc_all, test_size=(1-train_ratio), stratify=sc_all, random_state=42\n",
    "        )\n",
    "        vt_ratio = val_ratio / (1 - train_ratio)\n",
    "        X_val, X_te, y_val, y_te, sc_val, sc_te = train_test_split(\n",
    "            X_tmp, y_tmp, sc_tmp, test_size=(1-vt_ratio), stratify=sc_tmp, random_state=42\n",
    "        )\n",
    "        \n",
    "        splits = {\n",
    "            \"train\": {\"input\": X_tr, \"target\": y_tr, \"scenarios\": sc_tr},\n",
    "            \"validation\": {\"input\": X_val, \"target\": y_val, \"scenarios\": sc_val},\n",
    "            \"test\": {\"input\": X_te, \"target\": y_te, \"scenarios\": sc_te}\n",
    "        }\n",
    "        \n",
    "        loaders = {\n",
    "            \"train\": DataLoader(ClimateDatasetWithScenario(X_tr, y_tr, sc_tr), batch_size, True, \n",
    "                               pin_memory=False, num_workers=0),\n",
    "            \"validation\": DataLoader(ClimateDatasetWithScenario(X_val, y_val, sc_val), batch_size, False,\n",
    "                                    pin_memory=False, num_workers=0),\n",
    "            \"test\": DataLoader(ClimateDatasetWithScenario(X_te, y_te, sc_te), batch_size, False,\n",
    "                              pin_memory=False, num_workers=0)\n",
    "        }\n",
    "        \n",
    "        print(f\"   üì¶ Train:{len(X_tr)} Val:{len(X_val)} Test:{len(X_te)}\")\n",
    "        return splits, loaders\n",
    "\n",
    "\n",
    "print(\"‚úÖ Multi-scenario data loader defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2410f26",
   "metadata": {
    "papermill": {
     "duration": 0.006783,
     "end_time": "2025-10-23T18:43:40.101631",
     "exception": false,
     "start_time": "2025-10-23T18:43:40.094848",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## üß† Section 5: Spatio-Temporal GNN Architecture\n",
    "\n",
    "**Key Innovation: Unified spatio-temporal processing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "214a0474",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-23T18:43:40.117525Z",
     "iopub.status.busy": "2025-10-23T18:43:40.117197Z",
     "iopub.status.idle": "2025-10-23T18:43:40.142230Z",
     "shell.execute_reply": "2025-10-23T18:43:40.141240Z"
    },
    "papermill": {
     "duration": 0.03516,
     "end_time": "2025-10-23T18:43:40.143790",
     "exception": false,
     "start_time": "2025-10-23T18:43:40.108630",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ STGNN architecture defined\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# üß† SECTION 5: SPATIO-TEMPORAL GNN ARCHITECTURE\n",
    "# ============================================================================\n",
    "\n",
    "class TemporalConv(nn.Module):\n",
    "    \"\"\"\n",
    "    Temporal Convolution with causal padding.\n",
    "    Processes time dimension with 1D convolution.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, in_channels: int, out_channels: int, kernel_size: int = 3):\n",
    "        super().__init__()\n",
    "        self.kernel_size = kernel_size\n",
    "        # Causal padding: only look at past\n",
    "        self.padding = (kernel_size - 1)\n",
    "        self.conv = nn.Conv1d(in_channels, out_channels, kernel_size, padding=self.padding)\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: [batch, nodes, time, features]\n",
    "        Returns:\n",
    "            x: [batch, nodes, time, features]\n",
    "        \"\"\"\n",
    "        batch, nodes, time, features = x.shape\n",
    "        \n",
    "        # Reshape for 1D conv: [batch * nodes, features, time]\n",
    "        x = x.reshape(batch * nodes, time, features).permute(0, 2, 1)\n",
    "        \n",
    "        # Apply temporal convolution\n",
    "        x = self.conv(x)\n",
    "        \n",
    "        # Remove future timesteps (causal)\n",
    "        if self.padding > 0:\n",
    "            x = x[:, :, :-self.padding]\n",
    "        \n",
    "        # Reshape back: [batch, nodes, time, features]\n",
    "        x = x.permute(0, 2, 1).reshape(batch, nodes, time, features)\n",
    "        \n",
    "        return x\n",
    "\n",
    "\n",
    "class SpatialGraphConv(nn.Module):\n",
    "    \"\"\"\n",
    "    Spatial Graph Convolution.\n",
    "    Processes spatial dimension with graph convolution.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, in_channels: int, out_channels: int):\n",
    "        super().__init__()\n",
    "        self.conv = GCNConv(in_channels, out_channels)\n",
    "        self.bn = nn.BatchNorm1d(out_channels)\n",
    "    \n",
    "    def forward(self, x: torch.Tensor, edge_index: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: [batch * time, nodes, features]\n",
    "            edge_index: [2, num_edges]\n",
    "        Returns:\n",
    "            x: [batch * time, nodes, features]\n",
    "        \"\"\"\n",
    "        batch_time, nodes, features = x.shape\n",
    "        \n",
    "        # Reshape for graph conv: [batch_time * nodes, features]\n",
    "        x = x.reshape(batch_time * nodes, features)\n",
    "        \n",
    "        # Apply graph convolution\n",
    "        x = self.conv(x, edge_index)\n",
    "        x = self.bn(x)\n",
    "        x = F.relu(x)\n",
    "        \n",
    "        # Reshape back: [batch_time, nodes, features]\n",
    "        x = x.reshape(batch_time, nodes, features)\n",
    "        \n",
    "        return x\n",
    "\n",
    "\n",
    "class STGNNBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    Spatio-Temporal Graph Neural Network Block.\n",
    "    \n",
    "    Combines:\n",
    "    1. Temporal Convolution (captures temporal patterns)\n",
    "    2. Spatial Graph Convolution (captures spatial patterns)\n",
    "    3. Residual connection + Layer Norm\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, hidden_dim: int, temporal_kernel: int = 3, dropout: float = 0.2):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.temporal_conv = TemporalConv(hidden_dim, hidden_dim, temporal_kernel)\n",
    "        self.spatial_conv = SpatialGraphConv(hidden_dim, hidden_dim)\n",
    "        \n",
    "        self.layer_norm1 = nn.LayerNorm(hidden_dim)\n",
    "        self.layer_norm2 = nn.LayerNorm(hidden_dim)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        # FFN\n",
    "        self.ffn = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, hidden_dim * 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim * 2, hidden_dim),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "        self.layer_norm3 = nn.LayerNorm(hidden_dim)\n",
    "    \n",
    "    def forward(self, x: torch.Tensor, edge_index: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: [batch, nodes, time, features]\n",
    "            edge_index: [2, num_edges]\n",
    "        Returns:\n",
    "            x: [batch, nodes, time, features]\n",
    "        \"\"\"\n",
    "        batch, nodes, time, features = x.shape\n",
    "        \n",
    "        # 1. Temporal convolution with residual\n",
    "        identity = x\n",
    "        x_temporal = self.temporal_conv(x)\n",
    "        x_temporal = self.dropout(x_temporal)\n",
    "        x = self.layer_norm1(identity + x_temporal)\n",
    "        \n",
    "        # 2. Spatial graph convolution with residual\n",
    "        identity = x\n",
    "        # Reshape for spatial processing: [batch * time, nodes, features]\n",
    "        x_spatial = x.permute(0, 2, 1, 3).reshape(batch * time, nodes, features)\n",
    "        x_spatial = self.spatial_conv(x_spatial, edge_index)\n",
    "        x_spatial = self.dropout(x_spatial)\n",
    "        # Reshape back: [batch, nodes, time, features]\n",
    "        x_spatial = x_spatial.reshape(batch, time, nodes, features).permute(0, 2, 1, 3)\n",
    "        x = self.layer_norm2(identity + x_spatial)\n",
    "        \n",
    "        # 3. Feed-forward network with residual\n",
    "        identity = x\n",
    "        x_ffn = self.ffn(x)\n",
    "        x = self.layer_norm3(identity + x_ffn)\n",
    "        \n",
    "        return x\n",
    "\n",
    "\n",
    "class STGNNPredictor(nn.Module):\n",
    "    \"\"\"\n",
    "    Complete Spatio-Temporal GNN for Climate Prediction.\n",
    "    \n",
    "    Architecture:\n",
    "    1. Input embedding (8 channels ‚Üí hidden_dim)\n",
    "    2. Positional encoding\n",
    "    3. Multiple STGNN blocks\n",
    "    4. Temporal projection (12 ‚Üí 3 timesteps)\n",
    "    5. Output projection (hidden_dim ‚Üí 1)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self,\n",
    "                 input_channels: int = 8,\n",
    "                 hidden_dim: int = 64,\n",
    "                 num_blocks: int = 3,\n",
    "                 temporal_kernel: int = 3,\n",
    "                 spatial_size: Tuple[int, int] = (9, 19),\n",
    "                 input_length: int = 12,\n",
    "                 output_length: int = 3,\n",
    "                 dropout: float = 0.2):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.input_channels = input_channels\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_blocks = num_blocks\n",
    "        self.spatial_size = spatial_size\n",
    "        self.num_nodes = spatial_size[0] * spatial_size[1]\n",
    "        self.input_length = input_length\n",
    "        self.output_length = output_length\n",
    "        \n",
    "        print(f\"üîß Initializing STGNN Predictor:\")\n",
    "        print(f\"   Input: {input_length} timesteps √ó {input_channels} channels √ó {spatial_size[0]}√ó{spatial_size[1]}\")\n",
    "        print(f\"   Hidden: {hidden_dim}, Blocks: {num_blocks}\")\n",
    "        print(f\"   Output: {output_length} timesteps\")\n",
    "        \n",
    "        # Build graph structure\n",
    "        self.edge_index = build_grid_graph_8neighbor(spatial_size[0], spatial_size[1])\n",
    "        self.edge_index = add_self_loops(self.edge_index, self.num_nodes)\n",
    "        \n",
    "        # Positional encoding\n",
    "        self.pos_encoding = get_positional_encoding(spatial_size[0], spatial_size[1], hidden_dim)\n",
    "        \n",
    "        # Input embedding\n",
    "        self.input_embed = nn.Linear(input_channels, hidden_dim)\n",
    "        \n",
    "        # STGNN blocks\n",
    "        self.stgnn_blocks = nn.ModuleList([\n",
    "            STGNNBlock(hidden_dim, temporal_kernel, dropout)\n",
    "            for _ in range(num_blocks)\n",
    "        ])\n",
    "        \n",
    "        # Temporal projection: 12 timesteps ‚Üí 3 timesteps\n",
    "        # self.temporal_proj = nn.Conv1d(input_length, output_length, kernel_size=1)\n",
    "        self.temporal_proj = nn.Linear(self.input_length, self.output_length)\n",
    "        \n",
    "        # Output projection\n",
    "        self.output_proj = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, hidden_dim // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim // 2, 1)\n",
    "        )\n",
    "        \n",
    "        # Non-negative activation for precipitation\n",
    "        self.output_activation = nn.Softplus(beta=10)\n",
    "        \n",
    "        num_params = sum(p.numel() for p in self.parameters())\n",
    "        print(f\"‚úÖ STGNN initialized - Parameters: {num_params:,}\")\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: [batch, seq_len, channels, height, width]\n",
    "        Returns:\n",
    "            predictions: [batch, output_length, 1, height, width]\n",
    "        \"\"\"\n",
    "        batch_size, seq_len, channels, height, width = x.shape\n",
    "        edge_index = self.edge_index.to(x.device)\n",
    "        \n",
    "        # Reshape to [batch, nodes, time, channels]\n",
    "        x = x.permute(0, 1, 3, 4, 2)  # [batch, time, height, width, channels]\n",
    "        x = x.reshape(batch_size, seq_len, self.num_nodes, channels)\n",
    "        x = x.permute(0, 2, 1, 3)  # [batch, nodes, time, channels]\n",
    "        \n",
    "        # Input embedding\n",
    "        x = self.input_embed(x)  # [batch, nodes, time, hidden_dim]\n",
    "        \n",
    "        # Add positional encoding\n",
    "        pos_enc = self.pos_encoding.to(x.device)  # [nodes, hidden_dim]\n",
    "        pos_enc = pos_enc.unsqueeze(0).unsqueeze(2)  # [1, nodes, 1, hidden_dim]\n",
    "        x = x + pos_enc\n",
    "        \n",
    "        # Process through STGNN blocks\n",
    "        for block in self.stgnn_blocks:\n",
    "            x = block(x, edge_index)\n",
    "        \n",
    "        # x shape: [batch, nodes, time, hidden_dim]\n",
    "        \n",
    "        # Temporal projection: 12 ‚Üí 3 timesteps\n",
    "        # x_temp: [batch * nodes, time, hidden_dim]\n",
    "        x_temp = x.reshape(batch_size * self.num_nodes, seq_len, self.hidden_dim)\n",
    "        \n",
    "        # Permute to [batch*nodes, hidden_dim, time] so Linear acts on last dim=time\n",
    "        x_temp = x_temp.permute(0, 2, 1)  # [B*N, hidden_dim, seq_len]\n",
    "        \n",
    "        # Apply Linear to the time axis: Linear(seq_len -> output_length)\n",
    "        # The Linear acts on the last dimension (seq_len) and returns [B*N, hidden_dim, output_length]\n",
    "        x_temp = self.temporal_proj(x_temp)\n",
    "        \n",
    "        # Permute and reshape to [batch, nodes, output_length, hidden_dim]\n",
    "        x_temp = x_temp.permute(0, 2, 1).reshape(batch_size, self.num_nodes, self.output_length, self.hidden_dim)\n",
    "\n",
    "        \n",
    "        # Output projection\n",
    "        predictions = self.output_proj(x_temp)  # [batch, nodes, output_length, 1]\n",
    "        predictions = self.output_activation(predictions)\n",
    "        \n",
    "        # Reshape to [batch, output_length, 1, height, width]\n",
    "        predictions = predictions.squeeze(-1)  # [batch, nodes, output_length]\n",
    "        predictions = predictions.reshape(batch_size, height, width, self.output_length)\n",
    "        predictions = predictions.permute(0, 3, 1, 2).unsqueeze(2)  # [batch, output_length, 1, height, width]\n",
    "        \n",
    "        return predictions\n",
    "\n",
    "\n",
    "print(\"‚úÖ STGNN architecture defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6aa3cc17",
   "metadata": {
    "papermill": {
     "duration": 0.007657,
     "end_time": "2025-10-23T18:43:40.165412",
     "exception": false,
     "start_time": "2025-10-23T18:43:40.157755",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## üöÇ Section 6: Training Function\n",
    "\n",
    "*Updated for STGNN model*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fdf88df3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-23T18:43:40.182353Z",
     "iopub.status.busy": "2025-10-23T18:43:40.181994Z",
     "iopub.status.idle": "2025-10-23T18:43:40.215120Z",
     "shell.execute_reply": "2025-10-23T18:43:40.214061Z"
    },
    "papermill": {
     "duration": 0.043618,
     "end_time": "2025-10-23T18:43:40.216673",
     "exception": false,
     "start_time": "2025-10-23T18:43:40.173055",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Training function defined\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# üöÇ SECTION 6: TRAINING FUNCTION FOR STGNN\n",
    "# ============================================================================\n",
    "\n",
    "def train_single_institution_multi_scenario(institution: str) -> dict:\n",
    "    \"\"\"Train STGNN for single institution with multi-scenario data.\"\"\"\n",
    "    \n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"üåê Training STGNN: {institution}\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    model_path = os.path.join(OUTPUT_DIR, \"checkpoints\", f\"{institution}_stgnn_multiscenario_best.pt\")\n",
    "    \n",
    "    if RESUME_IF_MODEL_EXISTS and os.path.exists(model_path):\n",
    "        print(f\"‚≠ê Model exists - skipping {institution}\")\n",
    "        return {'success': True, 'institution': institution, 'skipped': True}\n",
    "    \n",
    "    try:\n",
    "        # Data loader\n",
    "        print(\"üìä Step 1/7: Initializing data loader...\")\n",
    "        dl = MultiScenarioDataLoader(target_h=TARGET_SPATIAL_H, target_w=TARGET_SPATIAL_W)\n",
    "        \n",
    "        # Discover files\n",
    "        print(\"üîç Step 2/7: Discovering files...\")\n",
    "        all_scenario_files = dl.discover_files_multi_scenario(institution)\n",
    "        \n",
    "        has_data = any(\n",
    "            bool(files['inputs']) or bool(files['outputs'])\n",
    "            for files in all_scenario_files.values()\n",
    "        )\n",
    "        \n",
    "        if not has_data:\n",
    "            print(f\"‚ö†Ô∏è No data for {institution}\")\n",
    "            return {'success': False, 'institution': institution, 'error': 'No data'}\n",
    "        \n",
    "        # Load scenarios\n",
    "        print(\"üìä Step 3/7: Loading data...\")\n",
    "        all_scenario_data = dl.load_all_scenarios(all_scenario_files)\n",
    "        \n",
    "        if not all_scenario_data:\n",
    "            print(f\"‚ö†Ô∏è Failed to load data\")\n",
    "            return {'success': False, 'institution': institution, 'error': 'Load failed'}\n",
    "        \n",
    "        # Create sequences\n",
    "        print(\"üîÑ Step 4/7: Creating sequences...\")\n",
    "        data_splits, dataloaders = dl.create_multi_scenario_sequences(all_scenario_data)\n",
    "        \n",
    "        # Create STGNN model\n",
    "        print(\"üß† Step 5/7: Creating STGNN model...\")\n",
    "        model = STGNNPredictor(\n",
    "            input_channels=NUM_INPUT_CHANNELS,\n",
    "            hidden_dim=HIDDEN_DIM,\n",
    "            num_blocks=STGNN_BLOCKS,\n",
    "            temporal_kernel=TEMPORAL_KERNEL_SIZE,\n",
    "            spatial_size=(TARGET_SPATIAL_H, TARGET_SPATIAL_W),\n",
    "            input_length=12,\n",
    "            output_length=3,\n",
    "            dropout=DROPOUT\n",
    "        ).to(device)\n",
    "        \n",
    "        optimizer = optim.AdamW(model.parameters(), lr=1e-4, weight_decay=0.05)\n",
    "        criterion = nn.MSELoss()\n",
    "        \n",
    "        history = {\n",
    "            'train_loss': [],\n",
    "            'val_loss': [],\n",
    "            'scenario_losses': {}\n",
    "        }\n",
    "        best_val_loss = float('inf')\n",
    "        \n",
    "        print(f\"\\nüöÇ Step 6/7: Training for {EPOCHS} epochs...\")\n",
    "        \n",
    "        epoch_pbar = tqdm(range(EPOCHS), desc=f\"üåê {institution}\", position=0, leave=True)\n",
    "        \n",
    "        for ep in epoch_pbar:\n",
    "            # Training\n",
    "            model.train()\n",
    "            train_loss = 0.0\n",
    "            train_batches = 0\n",
    "            \n",
    "            for X, y, scenarios in dataloaders['train']:\n",
    "                try:\n",
    "                    X, y = X.to(device), y.to(device)\n",
    "                    optimizer.zero_grad()\n",
    "                    preds = model(X)\n",
    "                    \n",
    "                    if preds.shape != y.shape:\n",
    "                        if preds.ndim == 4 and y.ndim == 5:\n",
    "                            preds = preds.unsqueeze(2)\n",
    "                    \n",
    "                    loss = criterion(preds, y)\n",
    "                    loss.backward()\n",
    "                    torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "                    optimizer.step()\n",
    "                    \n",
    "                    train_loss += loss.item()\n",
    "                    train_batches += 1\n",
    "                    \n",
    "                    if train_batches % 10 == 0:\n",
    "                        torch.cuda.empty_cache()\n",
    "                except Exception as e:\n",
    "                    print(f\"\\n‚ùå Training error: {e}\")\n",
    "                    continue\n",
    "            \n",
    "            avg_train_loss = train_loss / max(1, train_batches)\n",
    "            history['train_loss'].append(avg_train_loss)\n",
    "            \n",
    "            # Validation\n",
    "            model.eval()\n",
    "            val_loss = 0.0\n",
    "            val_batches = 0\n",
    "            scenario_losses = {s: [] for s in SCENARIOS.keys()}\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                for Xv, yv, scenarios_v in dataloaders['validation']:\n",
    "                    try:\n",
    "                        Xv, yv = Xv.to(device), yv.to(device)\n",
    "                        preds = model(Xv)\n",
    "                        \n",
    "                        if preds.shape != yv.shape:\n",
    "                            if preds.ndim == 4 and yv.ndim == 5:\n",
    "                                preds = preds.unsqueeze(2)\n",
    "                        \n",
    "                        batch_loss = criterion(preds, yv)\n",
    "                        val_loss += batch_loss.item()\n",
    "                        val_batches += 1\n",
    "                        \n",
    "                        # Track per-scenario\n",
    "                        for i, scenario_id in enumerate(scenarios_v.cpu().numpy()):\n",
    "                            scenario_name = [k for k, v in SCENARIOS.items() if v == scenario_id][0]\n",
    "                            scenario_losses[scenario_name].append(batch_loss.item())\n",
    "                    except Exception as e:\n",
    "                        print(f\"\\n‚ùå Validation error: {e}\")\n",
    "                        continue\n",
    "            \n",
    "            avg_val_loss = val_loss / max(1, val_batches)\n",
    "            history['val_loss'].append(avg_val_loss)\n",
    "            \n",
    "            avg_scenario_losses = {\n",
    "                s: np.mean(losses) if losses else 0.0\n",
    "                for s, losses in scenario_losses.items()\n",
    "            }\n",
    "            history['scenario_losses'][f'epoch_{ep+1}'] = avg_scenario_losses\n",
    "            \n",
    "            is_best = avg_val_loss < best_val_loss\n",
    "            if is_best:\n",
    "                best_val_loss = avg_val_loss\n",
    "                best_indicator = \"‚≠ê NEW BEST\"\n",
    "                \n",
    "                # ‚úÖ SAVE BEST MODEL NOW\n",
    "                print(f\"   üíæ Saving new best model (epoch {ep+1})...\")\n",
    "                sys.stdout.flush()\n",
    "                \n",
    "                try:\n",
    "                    torch.save({\n",
    "                        'epoch': ep + 1,\n",
    "                        'institution': institution,\n",
    "                        'model_state_dict': model.state_dict(),\n",
    "                        'optimizer_state_dict': optimizer.state_dict(),\n",
    "                        'best_val_loss': best_val_loss,\n",
    "                        'history': history,\n",
    "                        'spatial_dims': [TARGET_SPATIAL_H, TARGET_SPATIAL_W],\n",
    "                        'scenarios': list(SCENARIOS.keys()),\n",
    "                        'timestamp': datetime.now().isoformat()\n",
    "                    }, model_path)\n",
    "                    \n",
    "                    if os.path.exists(model_path):\n",
    "                        file_size = os.path.getsize(model_path) / (1024**2)\n",
    "                        print(f\"   ‚úÖ Best model saved ({file_size:.1f} MB)\")\n",
    "                    else:\n",
    "                        print(f\"   ‚ö†Ô∏è Model file not found after save\")\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    print(f\"   ‚ùå Error saving best model: {e}\")\n",
    "                \n",
    "                sys.stdout.flush()\n",
    "            else:\n",
    "                best_indicator = \"\"\n",
    "            \n",
    "            epoch_pbar.set_postfix({\n",
    "                'train': f'{avg_train_loss:.6f}',\n",
    "                'val': f'{avg_val_loss:.6f}',\n",
    "                'best': f'{best_val_loss:.6f}',\n",
    "                'status': best_indicator\n",
    "            })\n",
    "            \n",
    "            if ep % 3 == 0:\n",
    "                torch.cuda.empty_cache()\n",
    "                gc.collect()\n",
    "        \n",
    "        epoch_pbar.close()\n",
    "        \n",
    "        # ========================================\n",
    "        # üìä STEP 6.5: SAVE DETAILED METRICS\n",
    "        # ========================================\n",
    "        print(\"\\nüìä Step 6.5/8: Saving detailed metrics...\")\n",
    "        sys.stdout.flush()\n",
    "        \n",
    "        try:\n",
    "            import pandas as pd\n",
    "            \n",
    "            # Prepare epoch-by-epoch data\n",
    "            num_epochs = len(history['train_loss'])\n",
    "            metrics_data = {\n",
    "                'epoch': list(range(1, num_epochs + 1)),\n",
    "                'train_loss': history['train_loss'],\n",
    "                'val_loss': history['val_loss']\n",
    "            }\n",
    "            \n",
    "            # Add per-scenario losses per epoch\n",
    "            for epoch_key, scenario_losses in history.get('scenario_losses', {}).items():\n",
    "                epoch_num = int(epoch_key.split('_')[1])\n",
    "                for scenario, loss in scenario_losses.items():\n",
    "                    col_name = f'{scenario}_loss'\n",
    "                    if col_name not in metrics_data:\n",
    "                        metrics_data[col_name] = [None] * num_epochs\n",
    "                    metrics_data[col_name][epoch_num - 1] = loss\n",
    "            \n",
    "            # Create DataFrame\n",
    "            metrics_df = pd.DataFrame(metrics_data)\n",
    "            \n",
    "            # Save to CSV\n",
    "            metrics_path = os.path.join(OUTPUT_DIR, \"metrics\", f\"{institution}_stgnn_epoch_metrics.csv\")\n",
    "            metrics_df.to_csv(metrics_path, index=False, float_format='%.8f')\n",
    "            \n",
    "            # Verify\n",
    "            if os.path.exists(metrics_path):\n",
    "                file_size = os.path.getsize(metrics_path) / 1024\n",
    "                print(f\"‚úÖ Metrics CSV saved: {metrics_path} ({file_size:.1f} KB)\")\n",
    "                print(f\"   Columns: {', '.join(metrics_df.columns.tolist())}\")\n",
    "                print(f\"   Rows: {len(metrics_df)}\")\n",
    "            else:\n",
    "                print(f\"‚ö†Ô∏è Metrics file not found after save\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è Error saving metrics: {e}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "        \n",
    "        sys.stdout.flush()\n",
    "        \n",
    "        # ========================================\n",
    "        # üíæ STEP 7: SAVE FINAL MODEL\n",
    "        # ========================================\n",
    "        print(\"\\nüíæ Step 7/8: Saving final model...\")\n",
    "        sys.stdout.flush()\n",
    "        \n",
    "        final_path = os.path.join(OUTPUT_DIR, \"checkpoints\", f\"{institution}_stgnn_final.pt\")\n",
    "        \n",
    "        try:\n",
    "            torch.save({\n",
    "                'institution': institution,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'history': history,\n",
    "                'best_val_loss': best_val_loss,\n",
    "                'spatial_dims': [TARGET_SPATIAL_H, TARGET_SPATIAL_W],\n",
    "                'scenarios': list(SCENARIOS.keys()),\n",
    "                'timestamp': datetime.now().isoformat()\n",
    "            }, final_path)\n",
    "            \n",
    "            if os.path.exists(final_path):\n",
    "                file_size = os.path.getsize(final_path) / (1024**2)\n",
    "                print(f\"‚úÖ Final model saved: {final_path} ({file_size:.1f} MB)\")\n",
    "            else:\n",
    "                print(f\"‚ö†Ô∏è Final model file not found after save\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error saving final model: {e}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "        \n",
    "        sys.stdout.flush()\n",
    "        \n",
    "        # ========================================\n",
    "        # üíæ STEP 8: SAVE RESULTS JSON\n",
    "        # ========================================\n",
    "        print(\"\\nüíæ Step 8/8: Saving results JSON...\")\n",
    "        sys.stdout.flush()\n",
    "        \n",
    "        summary = {\n",
    "            'institution': institution,\n",
    "            'training_time': time.time() - start_time,\n",
    "            'epochs_trained': len(history['train_loss']),\n",
    "            'best_val_loss': best_val_loss,\n",
    "            'spatial_dims': [TARGET_SPATIAL_H, TARGET_SPATIAL_W],\n",
    "            'scenarios': list(SCENARIOS.keys()),\n",
    "            'final_scenario_losses': history['scenario_losses'].get(f'epoch_{EPOCHS}', {}),\n",
    "            'timestamp': datetime.now().isoformat()\n",
    "        }\n",
    "        \n",
    "        results_path = os.path.join(OUTPUT_DIR, \"logs\", f\"{institution}_stgnn_phase4_results.json\")\n",
    "        \n",
    "        try:\n",
    "            with open(results_path, 'w') as f:\n",
    "                json.dump(summary, f, indent=2)\n",
    "            \n",
    "            if os.path.exists(results_path):\n",
    "                file_size = os.path.getsize(results_path) / 1024\n",
    "                print(f\"‚úÖ Results JSON saved: {results_path} ({file_size:.1f} KB)\")\n",
    "            else:\n",
    "                print(f\"‚ö†Ô∏è Results file not found after save\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error saving results: {e}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "        \n",
    "        sys.stdout.flush()\n",
    "        \n",
    "        return {'success': True, 'institution': institution, 'results': summary}\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"\\n‚ùå FATAL ERROR: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return {'success': False, 'institution': institution, 'error': str(e)}\n",
    "    \n",
    "    finally:\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "\n",
    "\n",
    "print(\"‚úÖ Training function defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e3f0371",
   "metadata": {
    "papermill": {
     "duration": 0.007045,
     "end_time": "2025-10-23T18:43:40.231162",
     "exception": false,
     "start_time": "2025-10-23T18:43:40.224117",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## üöÄ Section 7: Main Training Loop\n",
    "\n",
    "*Same structure, updated for STGNN*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ebdaa55e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-23T18:43:40.247009Z",
     "iopub.status.busy": "2025-10-23T18:43:40.246701Z",
     "iopub.status.idle": "2025-10-23T19:17:37.234784Z",
     "shell.execute_reply": "2025-10-23T19:17:37.233467Z"
    },
    "papermill": {
     "duration": 2036.998332,
     "end_time": "2025-10-23T19:17:37.236732",
     "exception": false,
     "start_time": "2025-10-23T18:43:40.238400",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "üåê STGNN Phase 4 - Multi-Scenario Training\n",
      "================================================================================\n",
      "Start time: 2025-10-23 18:43:40\n",
      "‚ö†Ô∏è SMOKE TEST - Training only 2 institutions\n",
      "   Model: Spatio-Temporal GNN\n",
      "   Scenarios: ['historical', 'ssp126', 'ssp245', 'ssp370', 'ssp585']\n",
      "   Spatial: 9√ó19\n",
      "   STGNN Blocks: 3\n",
      "   Batch size: 1\n",
      "   Epochs: 2\n",
      "   Input channels: 8\n",
      "\n",
      "================================================================================\n",
      "üöÇ STARTING TRAINING\n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "üåê Overall:   0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "Institution 1/2: AWI-CM-1-1-MR\n",
      "================================================================================\n",
      "\n",
      "================================================================================\n",
      "üåê Training STGNN: AWI-CM-1-1-MR\n",
      "================================================================================\n",
      "üìä Step 1/7: Initializing data loader...\n",
      "üìä DataLoader initialized: 9√ó19\n",
      "üîç Step 2/7: Discovering files...\n",
      "üîç Discovering files for AWI-CM-1-1-MR...\n",
      "   üìÅ historical\n",
      "      ‚úì BC_anthro_fires: 165 files\n",
      "      ‚úì BC_no_fires: 165 files\n",
      "      ‚úì CH4_anthro_fires: 165 files\n",
      "      ‚úì CH4_no_fires: 165 files\n",
      "      ‚úì CO2_sum: 165 files\n",
      "      ‚úì SO2_anthro_fires: 165 files\n",
      "      ‚úì SO2_no_fires: 165 files\n",
      "      ‚úì pr: 165 files\n",
      "   üìÅ ssp126\n",
      "      ‚úì BC_anthro_fires: 86 files\n",
      "      ‚úì BC_no_fires: 86 files\n",
      "      ‚úì CH4_anthro_fires: 86 files\n",
      "      ‚úì CH4_no_fires: 86 files\n",
      "      ‚úì CO2_sum: 86 files\n",
      "      ‚úì SO2_anthro_fires: 86 files\n",
      "      ‚úì SO2_no_fires: 86 files\n",
      "      ‚úì pr: 86 files\n",
      "   üìÅ ssp245\n",
      "      ‚úì BC_anthro_fires: 86 files\n",
      "      ‚úì BC_no_fires: 86 files\n",
      "      ‚úì CH4_anthro_fires: 86 files\n",
      "      ‚úì CH4_no_fires: 86 files\n",
      "      ‚úì CO2_sum: 86 files\n",
      "      ‚úì SO2_anthro_fires: 86 files\n",
      "      ‚úì SO2_no_fires: 86 files\n",
      "      ‚úì pr: 86 files\n",
      "   üìÅ ssp370\n",
      "      ‚úì BC_anthro_fires: 86 files\n",
      "      ‚úì BC_no_fires: 86 files\n",
      "      ‚úì CH4_anthro_fires: 86 files\n",
      "      ‚úì CH4_no_fires: 86 files\n",
      "      ‚úì CO2_sum: 86 files\n",
      "      ‚úì SO2_anthro_fires: 86 files\n",
      "      ‚úì SO2_no_fires: 86 files\n",
      "      ‚úì pr: 86 files\n",
      "   üìÅ ssp585\n",
      "      ‚úì BC_anthro_fires: 86 files\n",
      "      ‚úì BC_no_fires: 86 files\n",
      "      ‚úì CH4_anthro_fires: 86 files\n",
      "      ‚úì CH4_no_fires: 86 files\n",
      "      ‚úì CO2_sum: 86 files\n",
      "      ‚úì SO2_anthro_fires: 86 files\n",
      "      ‚úì SO2_no_fires: 86 files\n",
      "      ‚úì pr: 86 files\n",
      "üìä Step 3/7: Loading data...\n",
      "üìä Loading scenarios...\n",
      "   üìÅ historical\n",
      "      ‚úì BC_anthro_fires: (1980, 96, 144)\n",
      "      ‚úì BC_no_fires: (1980, 96, 144)\n",
      "      ‚úì CH4_anthro_fires: (1980, 96, 144)\n",
      "      ‚úì CH4_no_fires: (1980, 96, 144)\n",
      "      ‚úì CO2_sum: (1980, 96, 144)\n",
      "      ‚úì SO2_anthro_fires: (1980, 96, 144)\n",
      "      ‚úì SO2_no_fires: (1980, 96, 144)\n",
      "      ‚úì pr: (1980, 96, 144)\n",
      "      üîΩ Downsampling...\n",
      "         BC_anthro_fires: (1980, 96, 144) ‚Üí (1980, 9, 19)\n",
      "         BC_no_fires: (1980, 96, 144) ‚Üí (1980, 9, 19)\n",
      "         CH4_anthro_fires: (1980, 96, 144) ‚Üí (1980, 9, 19)\n",
      "         CH4_no_fires: (1980, 96, 144) ‚Üí (1980, 9, 19)\n",
      "         CO2_sum: (1980, 96, 144) ‚Üí (1980, 9, 19)\n",
      "         SO2_anthro_fires: (1980, 96, 144) ‚Üí (1980, 9, 19)\n",
      "         SO2_no_fires: (1980, 96, 144) ‚Üí (1980, 9, 19)\n",
      "         pr: (1980, 96, 144) ‚Üí (1980, 9, 19)\n",
      "   üìÅ ssp126\n",
      "      ‚úì BC_anthro_fires: (1032, 96, 144)\n",
      "      ‚úì BC_no_fires: (1032, 96, 144)\n",
      "      ‚úì CH4_anthro_fires: (1032, 96, 144)\n",
      "      ‚úì CH4_no_fires: (1032, 96, 144)\n",
      "      ‚úì CO2_sum: (1032, 96, 144)\n",
      "      ‚úì SO2_anthro_fires: (1032, 96, 144)\n",
      "      ‚úì SO2_no_fires: (1032, 96, 144)\n",
      "      ‚úì pr: (1032, 96, 144)\n",
      "      üîΩ Downsampling...\n",
      "         BC_anthro_fires: (1032, 96, 144) ‚Üí (1032, 9, 19)\n",
      "         BC_no_fires: (1032, 96, 144) ‚Üí (1032, 9, 19)\n",
      "         CH4_anthro_fires: (1032, 96, 144) ‚Üí (1032, 9, 19)\n",
      "         CH4_no_fires: (1032, 96, 144) ‚Üí (1032, 9, 19)\n",
      "         CO2_sum: (1032, 96, 144) ‚Üí (1032, 9, 19)\n",
      "         SO2_anthro_fires: (1032, 96, 144) ‚Üí (1032, 9, 19)\n",
      "         SO2_no_fires: (1032, 96, 144) ‚Üí (1032, 9, 19)\n",
      "         pr: (1032, 96, 144) ‚Üí (1032, 9, 19)\n",
      "   üìÅ ssp245\n",
      "      ‚úì BC_anthro_fires: (1032, 96, 144)\n",
      "      ‚úì BC_no_fires: (1032, 96, 144)\n",
      "      ‚úì CH4_anthro_fires: (1032, 96, 144)\n",
      "      ‚úì CH4_no_fires: (1032, 96, 144)\n",
      "      ‚úì CO2_sum: (1032, 96, 144)\n",
      "      ‚úì SO2_anthro_fires: (1032, 96, 144)\n",
      "      ‚úì SO2_no_fires: (1032, 96, 144)\n",
      "      ‚úì pr: (1032, 96, 144)\n",
      "      üîΩ Downsampling...\n",
      "         BC_anthro_fires: (1032, 96, 144) ‚Üí (1032, 9, 19)\n",
      "         BC_no_fires: (1032, 96, 144) ‚Üí (1032, 9, 19)\n",
      "         CH4_anthro_fires: (1032, 96, 144) ‚Üí (1032, 9, 19)\n",
      "         CH4_no_fires: (1032, 96, 144) ‚Üí (1032, 9, 19)\n",
      "         CO2_sum: (1032, 96, 144) ‚Üí (1032, 9, 19)\n",
      "         SO2_anthro_fires: (1032, 96, 144) ‚Üí (1032, 9, 19)\n",
      "         SO2_no_fires: (1032, 96, 144) ‚Üí (1032, 9, 19)\n",
      "         pr: (1032, 96, 144) ‚Üí (1032, 9, 19)\n",
      "   üìÅ ssp370\n",
      "      ‚úì BC_anthro_fires: (1032, 96, 144)\n",
      "      ‚úì BC_no_fires: (1032, 96, 144)\n",
      "      ‚úì CH4_anthro_fires: (1032, 96, 144)\n",
      "      ‚úì CH4_no_fires: (1032, 96, 144)\n",
      "      ‚úì CO2_sum: (1032, 96, 144)\n",
      "      ‚úì SO2_anthro_fires: (1032, 96, 144)\n",
      "      ‚úì SO2_no_fires: (1032, 96, 144)\n",
      "      ‚úì pr: (1032, 96, 144)\n",
      "      üîΩ Downsampling...\n",
      "         BC_anthro_fires: (1032, 96, 144) ‚Üí (1032, 9, 19)\n",
      "         BC_no_fires: (1032, 96, 144) ‚Üí (1032, 9, 19)\n",
      "         CH4_anthro_fires: (1032, 96, 144) ‚Üí (1032, 9, 19)\n",
      "         CH4_no_fires: (1032, 96, 144) ‚Üí (1032, 9, 19)\n",
      "         CO2_sum: (1032, 96, 144) ‚Üí (1032, 9, 19)\n",
      "         SO2_anthro_fires: (1032, 96, 144) ‚Üí (1032, 9, 19)\n",
      "         SO2_no_fires: (1032, 96, 144) ‚Üí (1032, 9, 19)\n",
      "         pr: (1032, 96, 144) ‚Üí (1032, 9, 19)\n",
      "   üìÅ ssp585\n",
      "      ‚úì BC_anthro_fires: (1032, 96, 144)\n",
      "      ‚úì BC_no_fires: (1032, 96, 144)\n",
      "      ‚úì CH4_anthro_fires: (1032, 96, 144)\n",
      "      ‚úì CH4_no_fires: (1032, 96, 144)\n",
      "      ‚úì CO2_sum: (1032, 96, 144)\n",
      "      ‚úì SO2_anthro_fires: (1032, 96, 144)\n",
      "      ‚úì SO2_no_fires: (1032, 96, 144)\n",
      "      ‚úì pr: (1032, 96, 144)\n",
      "      üîΩ Downsampling...\n",
      "         BC_anthro_fires: (1032, 96, 144) ‚Üí (1032, 9, 19)\n",
      "         BC_no_fires: (1032, 96, 144) ‚Üí (1032, 9, 19)\n",
      "         CH4_anthro_fires: (1032, 96, 144) ‚Üí (1032, 9, 19)\n",
      "         CH4_no_fires: (1032, 96, 144) ‚Üí (1032, 9, 19)\n",
      "         CO2_sum: (1032, 96, 144) ‚Üí (1032, 9, 19)\n",
      "         SO2_anthro_fires: (1032, 96, 144) ‚Üí (1032, 9, 19)\n",
      "         SO2_no_fires: (1032, 96, 144) ‚Üí (1032, 9, 19)\n",
      "         pr: (1032, 96, 144) ‚Üí (1032, 9, 19)\n",
      "   ‚úÖ Loaded 5 scenarios\n",
      "üîÑ Step 4/7: Creating sequences...\n",
      "üîÑ Creating sequences...\n",
      "   üì¶ historical (id=0)\n",
      "      ‚úì 1966 sequences\n",
      "   üì¶ ssp126 (id=1)\n",
      "      ‚úì 1018 sequences\n",
      "   üì¶ ssp245 (id=2)\n",
      "      ‚úì 1018 sequences\n",
      "   üì¶ ssp370 (id=3)\n",
      "      ‚úì 1018 sequences\n",
      "   üì¶ ssp585 (id=4)\n",
      "      ‚úì 1018 sequences\n",
      "   üß© Total: X=(6038, 12, 8, 9, 19), Y=(6038, 3, 1, 9, 19)\n",
      "   üìä Stratified split...\n",
      "   üì¶ Train:4226 Val:905 Test:907\n",
      "üß† Step 5/7: Creating STGNN model...\n",
      "üîß Initializing STGNN Predictor:\n",
      "   Input: 12 timesteps √ó 8 channels √ó 9√ó19\n",
      "   Hidden: 64, Blocks: 3\n",
      "   Output: 3 timesteps\n",
      "üìä Graph: 9√ó19 = 171 nodes, 1204 edges\n",
      "   Avg degree: 7.04\n",
      "‚úÖ STGNN initialized - Parameters: 103,528\n",
      "\n",
      "üöÇ Step 6/7: Training for 2 epochs...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "üåê AWI-CM-1-1-MR:   0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   üíæ Saving new best model (epoch 1)...\n",
      "   ‚úÖ Best model saved (1.3 MB)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "üåê AWI-CM-1-1-MR: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [07:50<00:00, 235.18s/it, train=0.005015, val=0.008981, best=0.008470, status=]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Step 6.5/8: Saving detailed metrics...\n",
      "‚úÖ Metrics CSV saved: /kaggle/working/stgnn_phase4_multiscenario_results/metrics/AWI-CM-1-1-MR_stgnn_epoch_metrics.csv (0.2 KB)\n",
      "   Columns: epoch, train_loss, val_loss, historical_loss, ssp126_loss, ssp245_loss, ssp370_loss, ssp585_loss\n",
      "   Rows: 2\n",
      "\n",
      "üíæ Step 7/8: Saving final model...\n",
      "‚úÖ Final model saved: /kaggle/working/stgnn_phase4_multiscenario_results/checkpoints/AWI-CM-1-1-MR_stgnn_final.pt (1.3 MB)\n",
      "\n",
      "üíæ Step 8/8: Saving results JSON...\n",
      "‚úÖ Results JSON saved: /kaggle/working/stgnn_phase4_multiscenario_results/logs/AWI-CM-1-1-MR_stgnn_phase4_results.json (0.5 KB)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "üåê Overall:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1/2 [17:18<17:18, 1038.22s/it, success=1, skipped=0, failed=0]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "Institution 2/2: BCC-CSM2-MR\n",
      "================================================================================\n",
      "\n",
      "================================================================================\n",
      "üåê Training STGNN: BCC-CSM2-MR\n",
      "================================================================================\n",
      "üìä Step 1/7: Initializing data loader...\n",
      "üìä DataLoader initialized: 9√ó19\n",
      "üîç Step 2/7: Discovering files...\n",
      "üîç Discovering files for BCC-CSM2-MR...\n",
      "   üìÅ historical\n",
      "      ‚úì BC_anthro_fires: 165 files\n",
      "      ‚úì BC_no_fires: 165 files\n",
      "      ‚úì CH4_anthro_fires: 165 files\n",
      "      ‚úì CH4_no_fires: 165 files\n",
      "      ‚úì CO2_sum: 165 files\n",
      "      ‚úì SO2_anthro_fires: 165 files\n",
      "      ‚úì SO2_no_fires: 165 files\n",
      "      ‚úì pr: 165 files\n",
      "   üìÅ ssp126\n",
      "      ‚úì BC_anthro_fires: 86 files\n",
      "      ‚úì BC_no_fires: 86 files\n",
      "      ‚úì CH4_anthro_fires: 86 files\n",
      "      ‚úì CH4_no_fires: 86 files\n",
      "      ‚úì CO2_sum: 86 files\n",
      "      ‚úì SO2_anthro_fires: 86 files\n",
      "      ‚úì SO2_no_fires: 86 files\n",
      "      ‚úì pr: 86 files\n",
      "   üìÅ ssp245\n",
      "      ‚úì BC_anthro_fires: 86 files\n",
      "      ‚úì BC_no_fires: 86 files\n",
      "      ‚úì CH4_anthro_fires: 86 files\n",
      "      ‚úì CH4_no_fires: 86 files\n",
      "      ‚úì CO2_sum: 86 files\n",
      "      ‚úì SO2_anthro_fires: 86 files\n",
      "      ‚úì SO2_no_fires: 86 files\n",
      "      ‚úì pr: 86 files\n",
      "   üìÅ ssp370\n",
      "      ‚úì BC_anthro_fires: 86 files\n",
      "      ‚úì BC_no_fires: 86 files\n",
      "      ‚úì CH4_anthro_fires: 86 files\n",
      "      ‚úì CH4_no_fires: 86 files\n",
      "      ‚úì CO2_sum: 86 files\n",
      "      ‚úì SO2_anthro_fires: 86 files\n",
      "      ‚úì SO2_no_fires: 86 files\n",
      "      ‚úì pr: 86 files\n",
      "   üìÅ ssp585\n",
      "      ‚úì BC_anthro_fires: 86 files\n",
      "      ‚úì BC_no_fires: 86 files\n",
      "      ‚úì CH4_anthro_fires: 86 files\n",
      "      ‚úì CH4_no_fires: 86 files\n",
      "      ‚úì CO2_sum: 86 files\n",
      "      ‚úì SO2_anthro_fires: 86 files\n",
      "      ‚úì SO2_no_fires: 86 files\n",
      "      ‚úì pr: 86 files\n",
      "üìä Step 3/7: Loading data...\n",
      "üìä Loading scenarios...\n",
      "   üìÅ historical\n",
      "      ‚úì BC_anthro_fires: (1980, 96, 144)\n",
      "      ‚úì BC_no_fires: (1980, 96, 144)\n",
      "      ‚úì CH4_anthro_fires: (1980, 96, 144)\n",
      "      ‚úì CH4_no_fires: (1980, 96, 144)\n",
      "      ‚úì CO2_sum: (1980, 96, 144)\n",
      "      ‚úì SO2_anthro_fires: (1980, 96, 144)\n",
      "      ‚úì SO2_no_fires: (1980, 96, 144)\n",
      "      ‚úì pr: (1980, 96, 144)\n",
      "      üîΩ Downsampling...\n",
      "         BC_anthro_fires: (1980, 96, 144) ‚Üí (1980, 9, 19)\n",
      "         BC_no_fires: (1980, 96, 144) ‚Üí (1980, 9, 19)\n",
      "         CH4_anthro_fires: (1980, 96, 144) ‚Üí (1980, 9, 19)\n",
      "         CH4_no_fires: (1980, 96, 144) ‚Üí (1980, 9, 19)\n",
      "         CO2_sum: (1980, 96, 144) ‚Üí (1980, 9, 19)\n",
      "         SO2_anthro_fires: (1980, 96, 144) ‚Üí (1980, 9, 19)\n",
      "         SO2_no_fires: (1980, 96, 144) ‚Üí (1980, 9, 19)\n",
      "         pr: (1980, 96, 144) ‚Üí (1980, 9, 19)\n",
      "   üìÅ ssp126\n",
      "      ‚úì BC_anthro_fires: (1032, 96, 144)\n",
      "      ‚úì BC_no_fires: (1032, 96, 144)\n",
      "      ‚úì CH4_anthro_fires: (1032, 96, 144)\n",
      "      ‚úì CH4_no_fires: (1032, 96, 144)\n",
      "      ‚úì CO2_sum: (1032, 96, 144)\n",
      "      ‚úì SO2_anthro_fires: (1032, 96, 144)\n",
      "      ‚úì SO2_no_fires: (1032, 96, 144)\n",
      "      ‚úì pr: (1032, 96, 144)\n",
      "      üîΩ Downsampling...\n",
      "         BC_anthro_fires: (1032, 96, 144) ‚Üí (1032, 9, 19)\n",
      "         BC_no_fires: (1032, 96, 144) ‚Üí (1032, 9, 19)\n",
      "         CH4_anthro_fires: (1032, 96, 144) ‚Üí (1032, 9, 19)\n",
      "         CH4_no_fires: (1032, 96, 144) ‚Üí (1032, 9, 19)\n",
      "         CO2_sum: (1032, 96, 144) ‚Üí (1032, 9, 19)\n",
      "         SO2_anthro_fires: (1032, 96, 144) ‚Üí (1032, 9, 19)\n",
      "         SO2_no_fires: (1032, 96, 144) ‚Üí (1032, 9, 19)\n",
      "         pr: (1032, 96, 144) ‚Üí (1032, 9, 19)\n",
      "   üìÅ ssp245\n",
      "      ‚úì BC_anthro_fires: (1032, 96, 144)\n",
      "      ‚úì BC_no_fires: (1032, 96, 144)\n",
      "      ‚úì CH4_anthro_fires: (1032, 96, 144)\n",
      "      ‚úì CH4_no_fires: (1032, 96, 144)\n",
      "      ‚úì CO2_sum: (1032, 96, 144)\n",
      "      ‚úì SO2_anthro_fires: (1032, 96, 144)\n",
      "      ‚úì SO2_no_fires: (1032, 96, 144)\n",
      "      ‚úì pr: (1032, 96, 144)\n",
      "      üîΩ Downsampling...\n",
      "         BC_anthro_fires: (1032, 96, 144) ‚Üí (1032, 9, 19)\n",
      "         BC_no_fires: (1032, 96, 144) ‚Üí (1032, 9, 19)\n",
      "         CH4_anthro_fires: (1032, 96, 144) ‚Üí (1032, 9, 19)\n",
      "         CH4_no_fires: (1032, 96, 144) ‚Üí (1032, 9, 19)\n",
      "         CO2_sum: (1032, 96, 144) ‚Üí (1032, 9, 19)\n",
      "         SO2_anthro_fires: (1032, 96, 144) ‚Üí (1032, 9, 19)\n",
      "         SO2_no_fires: (1032, 96, 144) ‚Üí (1032, 9, 19)\n",
      "         pr: (1032, 96, 144) ‚Üí (1032, 9, 19)\n",
      "   üìÅ ssp370\n",
      "      ‚úì BC_anthro_fires: (1032, 96, 144)\n",
      "      ‚úì BC_no_fires: (1032, 96, 144)\n",
      "      ‚úì CH4_anthro_fires: (1032, 96, 144)\n",
      "      ‚úì CH4_no_fires: (1032, 96, 144)\n",
      "      ‚úì CO2_sum: (1032, 96, 144)\n",
      "      ‚úì SO2_anthro_fires: (1032, 96, 144)\n",
      "      ‚úì SO2_no_fires: (1032, 96, 144)\n",
      "      ‚úì pr: (1032, 96, 144)\n",
      "      üîΩ Downsampling...\n",
      "         BC_anthro_fires: (1032, 96, 144) ‚Üí (1032, 9, 19)\n",
      "         BC_no_fires: (1032, 96, 144) ‚Üí (1032, 9, 19)\n",
      "         CH4_anthro_fires: (1032, 96, 144) ‚Üí (1032, 9, 19)\n",
      "         CH4_no_fires: (1032, 96, 144) ‚Üí (1032, 9, 19)\n",
      "         CO2_sum: (1032, 96, 144) ‚Üí (1032, 9, 19)\n",
      "         SO2_anthro_fires: (1032, 96, 144) ‚Üí (1032, 9, 19)\n",
      "         SO2_no_fires: (1032, 96, 144) ‚Üí (1032, 9, 19)\n",
      "         pr: (1032, 96, 144) ‚Üí (1032, 9, 19)\n",
      "   üìÅ ssp585\n",
      "      ‚úì BC_anthro_fires: (1032, 96, 144)\n",
      "      ‚úì BC_no_fires: (1032, 96, 144)\n",
      "      ‚úì CH4_anthro_fires: (1032, 96, 144)\n",
      "      ‚úì CH4_no_fires: (1032, 96, 144)\n",
      "      ‚úì CO2_sum: (1032, 96, 144)\n",
      "      ‚úì SO2_anthro_fires: (1032, 96, 144)\n",
      "      ‚úì SO2_no_fires: (1032, 96, 144)\n",
      "      ‚úì pr: (1032, 96, 144)\n",
      "      üîΩ Downsampling...\n",
      "         BC_anthro_fires: (1032, 96, 144) ‚Üí (1032, 9, 19)\n",
      "         BC_no_fires: (1032, 96, 144) ‚Üí (1032, 9, 19)\n",
      "         CH4_anthro_fires: (1032, 96, 144) ‚Üí (1032, 9, 19)\n",
      "         CH4_no_fires: (1032, 96, 144) ‚Üí (1032, 9, 19)\n",
      "         CO2_sum: (1032, 96, 144) ‚Üí (1032, 9, 19)\n",
      "         SO2_anthro_fires: (1032, 96, 144) ‚Üí (1032, 9, 19)\n",
      "         SO2_no_fires: (1032, 96, 144) ‚Üí (1032, 9, 19)\n",
      "         pr: (1032, 96, 144) ‚Üí (1032, 9, 19)\n",
      "   ‚úÖ Loaded 5 scenarios\n",
      "üîÑ Step 4/7: Creating sequences...\n",
      "üîÑ Creating sequences...\n",
      "   üì¶ historical (id=0)\n",
      "      ‚úì 1966 sequences\n",
      "   üì¶ ssp126 (id=1)\n",
      "      ‚úì 1018 sequences\n",
      "   üì¶ ssp245 (id=2)\n",
      "      ‚úì 1018 sequences\n",
      "   üì¶ ssp370 (id=3)\n",
      "      ‚úì 1018 sequences\n",
      "   üì¶ ssp585 (id=4)\n",
      "      ‚úì 1018 sequences\n",
      "   üß© Total: X=(6038, 12, 8, 9, 19), Y=(6038, 3, 1, 9, 19)\n",
      "   üìä Stratified split...\n",
      "   üì¶ Train:4226 Val:905 Test:907\n",
      "üß† Step 5/7: Creating STGNN model...\n",
      "üîß Initializing STGNN Predictor:\n",
      "   Input: 12 timesteps √ó 8 channels √ó 9√ó19\n",
      "   Hidden: 64, Blocks: 3\n",
      "   Output: 3 timesteps\n",
      "üìä Graph: 9√ó19 = 171 nodes, 1204 edges\n",
      "   Avg degree: 7.04\n",
      "‚úÖ STGNN initialized - Parameters: 103,528\n",
      "\n",
      "üöÇ Step 6/7: Training for 2 epochs...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "üåê BCC-CSM2-MR:   0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   üíæ Saving new best model (epoch 1)...\n",
      "   ‚úÖ Best model saved (1.3 MB)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "üåê BCC-CSM2-MR: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [07:40<00:00, 230.40s/it, train=0.002190, val=0.003247, best=0.003192, status=]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Step 6.5/8: Saving detailed metrics...\n",
      "‚úÖ Metrics CSV saved: /kaggle/working/stgnn_phase4_multiscenario_results/metrics/BCC-CSM2-MR_stgnn_epoch_metrics.csv (0.2 KB)\n",
      "   Columns: epoch, train_loss, val_loss, historical_loss, ssp126_loss, ssp245_loss, ssp370_loss, ssp585_loss\n",
      "   Rows: 2\n",
      "\n",
      "üíæ Step 7/8: Saving final model...\n",
      "‚úÖ Final model saved: /kaggle/working/stgnn_phase4_multiscenario_results/checkpoints/BCC-CSM2-MR_stgnn_final.pt (1.3 MB)\n",
      "\n",
      "üíæ Step 8/8: Saving results JSON...\n",
      "‚úÖ Results JSON saved: /kaggle/working/stgnn_phase4_multiscenario_results/logs/BCC-CSM2-MR_stgnn_phase4_results.json (0.5 KB)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "üåê Overall: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [33:56<00:00, 1018.48s/it, success=2, skipped=0, failed=0]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "‚úÖ STGNN Phase 4 Training Complete!\n",
      "================================================================================\n",
      "Total: 2\n",
      "  ‚úÖ Successful: 2\n",
      "  ‚≠ê Skipped: 0\n",
      "  ‚ùå Failed: 0\n",
      "Total time: 0.57 hours\n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# üöÄ SECTION 7: MAIN TRAINING EXECUTION\n",
    "# ============================================================================\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"üåê STGNN Phase 4 - Multi-Scenario Training\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Start time: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "sys.stdout.flush()\n",
    "\n",
    "# Determine institutions\n",
    "if SMOKE_TEST:\n",
    "    institutions = ALL_INSTITUTIONS[:2]\n",
    "    print(f\"‚ö†Ô∏è SMOKE TEST - Training only {len(institutions)} institutions\")\n",
    "else:\n",
    "    institutions = ALL_INSTITUTIONS\n",
    "    print(f\"üìã Training all {len(institutions)} institutions\")\n",
    "\n",
    "print(f\"   Model: Spatio-Temporal GNN\")\n",
    "print(f\"   Scenarios: {list(SCENARIOS.keys())}\")\n",
    "print(f\"   Spatial: {TARGET_SPATIAL_H}√ó{TARGET_SPATIAL_W}\")\n",
    "print(f\"   STGNN Blocks: {STGNN_BLOCKS}\")\n",
    "print(f\"   Batch size: {BATCH_SIZE}\")\n",
    "print(f\"   Epochs: {EPOCHS}\")\n",
    "print(f\"   Input channels: {NUM_INPUT_CHANNELS}\")\n",
    "sys.stdout.flush()\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"\\nüéÆ GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"   Memory: {torch.cuda.get_device_properties(0).total_memory / (1024**3):.1f} GB\")\n",
    "\n",
    "if SKIP_TRAINING:\n",
    "    print(\"SKIP_TRAINING=True - Loading existing results...\")\n",
    "    summary_path = os.path.join(OUTPUT_DIR, \"logs\", \"stgnn_phase4_training_summary.json\")\n",
    "    if os.path.exists(summary_path):\n",
    "        with open(summary_path, 'r') as f:\n",
    "            summary = json.load(f)\n",
    "        print(\"‚úÖ Loaded existing results\")\n",
    "    else:\n",
    "        summary = {}\n",
    "else:\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"üöÇ STARTING TRAINING\")\n",
    "    print(\"=\"*80)\n",
    "    sys.stdout.flush()\n",
    "    \n",
    "    t0 = time.time()\n",
    "    all_results = []\n",
    "    \n",
    "    main_pbar = tqdm(institutions, desc=\"üåê Overall\", position=0)\n",
    "    \n",
    "    for inst_idx, institution in enumerate(main_pbar):\n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(f\"Institution {inst_idx+1}/{len(institutions)}: {institution}\")\n",
    "        print(f\"{'='*80}\")\n",
    "        sys.stdout.flush()\n",
    "        \n",
    "        try:\n",
    "            result = train_single_institution_multi_scenario(institution)\n",
    "            all_results.append(result)\n",
    "            \n",
    "            successful = sum(1 for r in all_results if r.get('success') and not r.get('skipped'))\n",
    "            skipped = sum(1 for r in all_results if r.get('skipped'))\n",
    "            failed = sum(1 for r in all_results if not r.get('success'))\n",
    "            \n",
    "            main_pbar.set_postfix({\n",
    "                'success': successful,\n",
    "                'skipped': skipped,\n",
    "                'failed': failed\n",
    "            })\n",
    "            \n",
    "            # Save progress\n",
    "            progress = {\n",
    "                'completed': len(all_results),\n",
    "                'total': len(institutions),\n",
    "                'results': all_results,\n",
    "                'timestamp': datetime.now().isoformat()\n",
    "            }\n",
    "            \n",
    "            progress_path = os.path.join(OUTPUT_DIR, \"logs\", \"stgnn_phase4_progress.json\")\n",
    "            with open(progress_path, 'w') as f:\n",
    "                json.dump(progress, f, indent=2)\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"\\n‚ùå CRITICAL ERROR: {e}\")\n",
    "            all_results.append({\n",
    "                'success': False,\n",
    "                'institution': institution,\n",
    "                'error': str(e)\n",
    "            })\n",
    "    \n",
    "    main_pbar.close()\n",
    "    elapsed = time.time() - t0\n",
    "    \n",
    "    # Summary\n",
    "    successful = [r for r in all_results if r.get('success') and not r.get('skipped')]\n",
    "    skipped = [r for r in all_results if r.get('skipped')]\n",
    "    failed = [r for r in all_results if not r.get('success')]\n",
    "    \n",
    "    summary = {\n",
    "        'model_type': 'STGNN',\n",
    "        'phase': 'phase4_multi_scenario',\n",
    "        'total_institutions': len(institutions),\n",
    "        'successful': len(successful),\n",
    "        'skipped': len(skipped),\n",
    "        'failed': len(failed),\n",
    "        'total_time_hours': elapsed / 3600,\n",
    "        'scenarios': list(SCENARIOS.keys()),\n",
    "        'results': all_results,\n",
    "        'timestamp': datetime.now().isoformat()\n",
    "    }\n",
    "    \n",
    "    summary_path = os.path.join(OUTPUT_DIR, \"logs\", \"stgnn_phase4_training_summary.json\")\n",
    "    with open(summary_path, 'w') as f:\n",
    "        json.dump(summary, f, indent=2)\n",
    "    \n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(\"‚úÖ STGNN Phase 4 Training Complete!\")\n",
    "    print(f\"{'='*80}\")\n",
    "    print(f\"Total: {len(institutions)}\")\n",
    "    print(f\"  ‚úÖ Successful: {len(successful)}\")\n",
    "    print(f\"  ‚≠ê Skipped: {len(skipped)}\")\n",
    "    print(f\"  ‚ùå Failed: {len(failed)}\")\n",
    "    print(f\"Total time: {elapsed/3600:.2f} hours\")\n",
    "    print(f\"{'='*80}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44ce064f",
   "metadata": {
    "papermill": {
     "duration": 0.017015,
     "end_time": "2025-10-23T19:17:37.271731",
     "exception": false,
     "start_time": "2025-10-23T19:17:37.254716",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## üìä Section 8: Results Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "82cb1de9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-23T19:17:37.307763Z",
     "iopub.status.busy": "2025-10-23T19:17:37.307404Z",
     "iopub.status.idle": "2025-10-23T19:17:38.040780Z",
     "shell.execute_reply": "2025-10-23T19:17:38.039575Z"
    },
    "papermill": {
     "duration": 0.753658,
     "end_time": "2025-10-23T19:17:38.042394",
     "exception": false,
     "start_time": "2025-10-23T19:17:37.288736",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "üìä STGNN PHASE 4 RESULTS SUMMARY\n",
      "================================================================================\n",
      "Model Type: STGNN\n",
      "Total institutions: 2\n",
      "  ‚úÖ Successful: 2\n",
      "  ‚≠ê Skipped: 0\n",
      "  ‚ùå Failed: 0\n",
      "Total time: 0.57h\n",
      "Scenarios: ['historical', 'ssp126', 'ssp245', 'ssp370', 'ssp585']\n",
      "\n",
      "üåç Scenario Performance Across All Institutions:\n",
      "================================================================================\n",
      "   HISTORICAL  : Avg=0.006248¬±0.002534 | Min=0.003714 | Max=0.008782\n",
      "   SSP126      : Avg=0.006954¬±0.003942 | Min=0.003012 | Max=0.010896\n",
      "   SSP245      : Avg=0.005222¬±0.002138 | Min=0.003085 | Max=0.007360\n",
      "   SSP370      : Avg=0.005649¬±0.002774 | Min=0.002874 | Max=0.008423\n",
      "   SSP585      : Avg=0.006368¬±0.003251 | Min=0.003117 | Max=0.009619\n",
      "================================================================================\n",
      "‚úÖ Plot saved: /kaggle/working/stgnn_phase4_multiscenario_results/plots/stgnn_scenario_performance.png (131.5 KB)\n",
      "\n",
      "üìä Plot saved to: /kaggle/working/stgnn_phase4_multiscenario_results/plots/stgnn_scenario_performance.png\n",
      "\n",
      "‚úÖ Results saved to: /kaggle/working/stgnn_phase4_multiscenario_results\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# üìä SECTION 8: RESULTS ANALYSIS\n",
    "# ============================================================================\n",
    "\n",
    "if summary:\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"üìä STGNN PHASE 4 RESULTS SUMMARY\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"Model Type: {summary.get('model_type', 'STGNN')}\")\n",
    "    print(f\"Total institutions: {summary.get('total_institutions', 0)}\")\n",
    "    print(f\"  ‚úÖ Successful: {summary.get('successful', 0)}\")\n",
    "    print(f\"  ‚≠ê Skipped: {summary.get('skipped', 0)}\")\n",
    "    print(f\"  ‚ùå Failed: {summary.get('failed', 0)}\")\n",
    "    print(f\"Total time: {summary.get('total_time_hours', 0):.2f}h\")\n",
    "    print(f\"Scenarios: {summary.get('scenarios', [])}\")\n",
    "    \n",
    "    # Scenario performance\n",
    "    print(f\"\\nüåç Scenario Performance Across All Institutions:\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    scenario_perf = {s: [] for s in SCENARIOS.keys()}\n",
    "    \n",
    "    for result in summary.get('results', []):\n",
    "        if result.get('success') and not result.get('skipped'):\n",
    "            inst_results = result.get('results', {})\n",
    "            final_losses = inst_results.get('final_scenario_losses', {})\n",
    "            \n",
    "            for scenario, loss in final_losses.items():\n",
    "                if scenario in scenario_perf:\n",
    "                    scenario_perf[scenario].append(loss)\n",
    "    \n",
    "    for scenario, losses in scenario_perf.items():\n",
    "        if losses:\n",
    "            avg = np.mean(losses)\n",
    "            std = np.std(losses)\n",
    "            min_loss = np.min(losses)\n",
    "            max_loss = np.max(losses)\n",
    "            print(f\"   {scenario.upper():12s}: Avg={avg:.6f}¬±{std:.6f} | Min={min_loss:.6f} | Max={max_loss:.6f}\")\n",
    "        else:\n",
    "            print(f\"   {scenario.upper():12s}: No data\")\n",
    "    \n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Visualization\n",
    "    if scenario_perf and any(scenario_perf.values()):\n",
    "        fig, ax = plt.subplots(figsize=(12, 6))\n",
    "        \n",
    "        scenarios_with_data = [s for s, losses in scenario_perf.items() if losses]\n",
    "        avg_losses = [np.mean(scenario_perf[s]) for s in scenarios_with_data]\n",
    "        std_losses = [np.std(scenario_perf[s]) for s in scenarios_with_data]\n",
    "        \n",
    "        x = np.arange(len(scenarios_with_data))\n",
    "        ax.bar(x, avg_losses, yerr=std_losses, capsize=5, alpha=0.7, color='teal')\n",
    "        ax.set_xlabel('Scenario', fontsize=12)\n",
    "        ax.set_ylabel('Average Loss', fontsize=12)\n",
    "        ax.set_title('STGNN Performance Across Climate Scenarios', fontsize=14, fontweight='bold')\n",
    "        ax.set_xticks(x)\n",
    "        ax.set_xticklabels([s.upper() for s in scenarios_with_data], rotation=45)\n",
    "        ax.grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        \n",
    "        # Save plot with verification\n",
    "        plot_path = os.path.join(OUTPUT_DIR, \"plots\", \"stgnn_scenario_performance.png\")\n",
    "        \n",
    "        try:\n",
    "            plt.savefig(plot_path, dpi=300, bbox_inches='tight')\n",
    "            plt.close()\n",
    "            \n",
    "            if os.path.exists(plot_path):\n",
    "                file_size = os.path.getsize(plot_path) / 1024\n",
    "                print(f\"‚úÖ Plot saved: {plot_path} ({file_size:.1f} KB)\")\n",
    "            else:\n",
    "                print(f\"‚ö†Ô∏è Plot file not found after save\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error saving plot: {e}\")\n",
    "            plt.close()\n",
    "        \n",
    "        print(f\"\\nüìä Plot saved to: {OUTPUT_DIR}/plots/stgnn_scenario_performance.png\")\n",
    "\n",
    "print(f\"\\n‚úÖ Results saved to: {OUTPUT_DIR}\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06ae211f",
   "metadata": {
    "papermill": {
     "duration": 0.017467,
     "end_time": "2025-10-23T19:17:38.080117",
     "exception": false,
     "start_time": "2025-10-23T19:17:38.062650",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "---\n",
    "\n",
    "## üéâ Complete!\n",
    "\n",
    "### üåü What's Different: STGNN vs GNN-GRU\n",
    "\n",
    "| Component | **Previous (GNN-GRU)** | **Now (STGNN)** |\n",
    "|-----------|----------------------|----------------|\n",
    "| **Architecture** | Separate spatial (GNN) + temporal (GRU) | Unified spatio-temporal blocks |\n",
    "| **Temporal** | GRU (sequential) | Temporal convolution (parallel) |\n",
    "| **Processing** | Sequential over time | Processes all timesteps together |\n",
    "| **Efficiency** | Moderate (GRU overhead) | Higher (parallel processing) |\n",
    "| **Parameters** | ~1-1.5M | ~800K-1.2M |\n",
    "| **Training Speed** | Slower (sequential) | Faster (parallel) |\n",
    "| **Climate Data** | Good | Better (captures ST patterns) |\n",
    "\n",
    "### üî¨ STGNN Architecture:\n",
    "```\n",
    "Input (12√ó8√ó9√ó19)\n",
    "    ‚Üì\n",
    "[Graph: 8-neighbor connectivity]\n",
    "    ‚Üì\n",
    "[Input Embed: 8 ‚Üí 64]\n",
    "[+ Positional Encoding]\n",
    "    ‚Üì\n",
    "‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó\n",
    "‚ïë   STGNN Block √ó 3                 ‚ïë\n",
    "‚ïë   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚ïë\n",
    "‚ïë   ‚îÇ Temporal Conv (1D)      ‚îÇ    ‚ïë\n",
    "‚ïë   ‚îÇ ‚Üì                       ‚îÇ    ‚ïë\n",
    "‚ïë   ‚îÇ Graph Conv (Spatial)    ‚îÇ    ‚ïë\n",
    "‚ïë   ‚îÇ ‚Üì                       ‚îÇ    ‚ïë\n",
    "‚ïë   ‚îÇ Feed-Forward Network    ‚îÇ    ‚ïë\n",
    "‚ïë   ‚îÇ ‚Üì                       ‚îÇ    ‚ïë\n",
    "‚ïë   ‚îÇ Layer Norm + Residual   ‚îÇ    ‚ïë\n",
    "‚ïë   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚ïë\n",
    "‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù\n",
    "    ‚Üì\n",
    "[Temporal Projection: 12 ‚Üí 3]\n",
    "    ‚Üì\n",
    "[Output Projection: 64 ‚Üí 1]\n",
    "[+ Softplus]\n",
    "    ‚Üì\n",
    "Output (3√ó1√ó9√ó19)\n",
    "```\n",
    "\n",
    "### üéØ Key Advantages of STGNN:\n",
    "\n",
    "#### **1. Unified Processing**\n",
    "- Processes spatial and temporal together\n",
    "- No separation between spatial and temporal\n",
    "- More natural for climate data\n",
    "\n",
    "#### **2. Parallel Temporal Processing**\n",
    "```python\n",
    "# GNN-GRU (Sequential)\n",
    "for t in timesteps:\n",
    "    hidden = gru_cell(x[t], hidden)  # One at a time\n",
    "\n",
    "# STGNN (Parallel)\n",
    "x_all_time = temporal_conv(x)  # All at once!\n",
    "```\n",
    "\n",
    "#### **3. Causal Temporal Convolutions**\n",
    "- Only looks at past (causal padding)\n",
    "- Respects temporal ordering\n",
    "- Prevents information leakage\n",
    "\n",
    "#### **4. Better for Climate**\n",
    "- Climate has strong spatio-temporal correlations\n",
    "- STGNN captures these naturally\n",
    "- No artificial separation\n",
    "\n",
    "### üìä Expected Performance:\n",
    "\n",
    "| Metric | STGNN Expected | Previous GNN-GRU |\n",
    "|--------|---------------|------------------|\n",
    "| **Training Speed** | **~30-40 min/epoch** | ~45-60 min/epoch |\n",
    "| **Memory** | **~3-4 GB** | ~4-5 GB |\n",
    "| **Parameters** | **~800K-1.2M** | ~1-1.5M |\n",
    "| **MAE** | **0.0003-0.0008** | 0.0004-0.0010 |\n",
    "| **R¬≤** | **0.7-0.92** | 0.65-0.88 |\n",
    "\n",
    "### üîß Technical Details:\n",
    "\n",
    "#### **Temporal Convolution**\n",
    "```python\n",
    "# 1D convolution over time\n",
    "# Kernel size 3 = looks at [t-2, t-1, t]\n",
    "# Causal padding = no future information\n",
    "```\n",
    "\n",
    "#### **Spatio-Temporal Block**\n",
    "```python\n",
    "1. Temporal Conv ‚Üí captures time patterns\n",
    "2. Graph Conv ‚Üí captures spatial patterns  \n",
    "3. FFN ‚Üí non-linear transformations\n",
    "4. Layer Norm + Residual ‚Üí stable training\n",
    "```\n",
    "\n",
    "#### **Temporal Projection**\n",
    "```python\n",
    "# Projects 12 input timesteps ‚Üí 3 output timesteps\n",
    "# Learnable Conv1d(12, 3)\n",
    "# More flexible than taking last 3\n",
    "```\n",
    "\n",
    "### üìÅ Output Files:\n",
    "- **Models**: `stgnn_phase4_multiscenario_results/checkpoints/{inst}_stgnn_multiscenario_best.pt`\n",
    "- **Logs**: `stgnn_phase4_multiscenario_results/logs/{inst}_stgnn_phase4_results.json`\n",
    "- **Summary**: `stgnn_phase4_multiscenario_results/logs/stgnn_phase4_training_summary.json`\n",
    "- **Plots**: `stgnn_phase4_multiscenario_results/plots/stgnn_scenario_performance.png`\n",
    "\n",
    "### üöÄ Ready to Compare!\n",
    "\n",
    "Now you have:\n",
    "1. ‚úÖ ConvLSTM (CNN + LSTM)\n",
    "2. ‚úÖ ClimAx (Vision Transformer)\n",
    "3. ‚úÖ **STGNN** (Unified spatio-temporal graph)\n",
    "\n",
    "**Perfect for comprehensive comparison!**"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 8432230,
     "sourceId": 13303078,
     "sourceType": "datasetVersion"
    }
   ],
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 2075.091572,
   "end_time": "2025-10-23T19:17:40.837689",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-10-23T18:43:05.746117",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
